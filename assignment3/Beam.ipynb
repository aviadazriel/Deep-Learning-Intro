{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Embedding, CuDNNLSTM, Bidirectional, Dense, CuDNNGRU\n",
    "from keras.initializers import Constant\n",
    "from keras import Sequential\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "from keras.backend import epsilon\n",
    "import ipdb\n",
    "\n",
    "EMBEDDING_DIM = 300\n",
    "INPUT_LENGTH = 50\n",
    "\n",
    "\n",
    "class Model:\n",
    "    def __init__(self, tokenizer, embedding_matrix,\n",
    "                 rnn_units=50,\n",
    "                 bidirectional=True,\n",
    "                 rnn_type='lstm',\n",
    "                 show_summary=True):\n",
    "        rnn_types = {\n",
    "            'lstm': CuDNNLSTM,\n",
    "            'gru': CuDNNGRU\n",
    "        }\n",
    "        rnn_type = rnn_types[rnn_type]\n",
    "\n",
    "        # load pre-trained word embeddings into an Embedding layer\n",
    "        # note that we set trainable = False so as to keep the embeddings fixed\n",
    "        num_words = len(tokenizer.word_index) + 1\n",
    "        embedding_layer = Embedding(num_words,\n",
    "                                    EMBEDDING_DIM,\n",
    "                                    embeddings_initializer=Constant(embedding_matrix),\n",
    "                                    input_length=INPUT_LENGTH,\n",
    "                                    trainable=False)\n",
    "        model = Sequential()\n",
    "        model.add(embedding_layer)\n",
    "        if bidirectional:\n",
    "            model.add(Bidirectional(rnn_type(rnn_units)))\n",
    "        else:\n",
    "            model.add(rnn_type(rnn_units))\n",
    "        model.add(Dense(num_words, activation='softmax'))\n",
    "        if show_summary:\n",
    "            model.summary()\n",
    "\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def train(self, X, y, epochs=5, batch_size=32, callbacks=[]):\n",
    "        model = self.model\n",
    "        # compile network\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[_perplexity])\n",
    "        # fit network\n",
    "        model.fit(X, y, \n",
    "                  epochs=epochs,\n",
    "                  batch_size=batch_size,\n",
    "                  verbose=1, \n",
    "                  shuffle=True,\n",
    "                  validation_split=0.2,\n",
    "                  callbacks=callbacks)\n",
    "\n",
    "    def predict(self, first_word, n_words, B=3):\n",
    "        tokenizer = self.tokenizer\n",
    "        model = self.model\n",
    "        in_text, result = first_word, [first_word]\n",
    "        encoded = get_encoded(in_text, tokenizer)\n",
    "        beam_sequences_scores = [[encoded, 0]]\n",
    "\n",
    "        while len(result) < n_words:\n",
    "            all_candidates = []\n",
    "            beam_sequences_scores = self.beam_step(beam_sequences_scores, B)\n",
    "            for seq_score in beam_sequences_scores:\n",
    "                seq_scores = self.beam_step([seq_score], B)\n",
    "                all_candidates.append(seq_scores)\n",
    "            flatten = lambda lst: [item for sublist in lst for item in sublist]\n",
    "            beam_sequences_scores = sorted(flatten(all_candidates), reverse=True, key=lambda tup: tup[1])[:B]\n",
    "            result, _= beam_sequences_scores[0]\n",
    "        \n",
    "        words = [get_word(token, self.tokenizer) for token in result]\n",
    "        return ' '.join(words)\n",
    "    \n",
    "    \n",
    "    def beam_step(self, beam_sequences_scores, B):            \n",
    "        all_candidates = []\n",
    "        for seq, score in beam_sequences_scores: # for each sequence\n",
    "            # predict top B words\n",
    "            seq_pad = pad_sequences([[sample] for sample in seq], maxlen=INPUT_LENGTH)\n",
    "            words_probs = self.model.predict_proba(seq_pad, verbose=0)[0]\n",
    "            words_probs_enu = list(enumerate(words_probs))\n",
    "            words_probs_sorted = sorted(words_probs_enu, key=lambda x: x[1], reverse=True) # sorting in descending order\n",
    "            top_b_words_probs = words_probs_sorted[:B] # top B words with max probability\n",
    "            # for each prob in top B words, create a candidate\n",
    "            for token, prob in top_b_words_probs: \n",
    "                word_token = token\n",
    "                candidate = [np.append(seq, word_token), score + np.log(prob + epsilon())] # todo: word_token \n",
    "                all_candidates.append(candidate)\n",
    "        # take candidates with max score\n",
    "        beam_sequences_scores = sorted(all_candidates, reverse=True, key=lambda tup: tup[1])[:B]\n",
    "        return beam_sequences_scores\n",
    "\n",
    "def get_encoded(text, tokenizer):\n",
    "    encoded = tokenizer.texts_to_sequences([text])[0]\n",
    "    encoded = np.array(encoded)\n",
    "    return encoded\n",
    "\n",
    "def get_word(index, tokenizer):\n",
    "    for word, idx in tokenizer.word_index.items():\n",
    "         if idx == index:\n",
    "            return word\n",
    "\n",
    "def _perplexity(y_true, y_pred):\n",
    "    cross_entropy = K.categorical_crossentropy(y_true, y_pred)\n",
    "    perplexity = K.pow(2.0, cross_entropy)\n",
    "    return perplexity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import src.dataset as ds\n",
    "import numpy as np\n",
    "from src.embeddings import extract_embedding_weights\n",
    "from keras.layers import Embedding, CuDNNLSTM, Bidirectional, Dense, CuDNNGRU\n",
    "from keras.initializers import Constant\n",
    "from keras import Sequential\n",
    "from tensorflow.python.keras.callbacks import TensorBoard\n",
    "# from src.model import Model\n",
    "\n",
    "from time import time\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 615/615 [00:01<00:00, 354.19it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 615/615 [00:01<00:00, 355.20it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 615/615 [00:01<00:00, 351.76it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 615/615 [00:01<00:00, 363.80it/s]\n"
     ]
    }
   ],
   "source": [
    "X, y, tokenizer = ds.load_tokenized_data()\n",
    "embedding_matrix = extract_embedding_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "X = pad_sequences([[sample] for sample in X], maxlen=INPUT_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Eli\\Anaconda3\\envs\\dlinto\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 50, 300)           2251800   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 300)               406800    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 7506)              2259306   \n",
      "=================================================================\n",
      "Total params: 4,917,906\n",
      "Trainable params: 2,666,106\n",
      "Non-trainable params: 2,251,800\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model(tokenizer, embedding_matrix, rnn_units=150, rnn_type='gru')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Eli\\Anaconda3\\envs\\dlinto\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 154596 samples, validate on 38650 samples\n",
      "Epoch 1/10\n",
      "154596/154596 [==============================] - 90s 581us/step - loss: 4.9001 - _perplexity: 867.2607 - val_loss: 5.3807 - val__perplexity: 2141.3858\n",
      "Epoch 2/10\n",
      "154596/154596 [==============================] - 82s 529us/step - loss: 4.3700 - _perplexity: 493.4785 - val_loss: 5.4135 - val__perplexity: 3428.7785\n",
      "Epoch 3/10\n",
      "154596/154596 [==============================] - 83s 540us/step - loss: 4.2017 - _perplexity: 401.3853 - val_loss: 5.4521 - val__perplexity: 4138.1159\n",
      "Epoch 4/10\n",
      "154596/154596 [==============================] - 84s 546us/step - loss: 4.1103 - _perplexity: 374.2492 - val_loss: 5.4633 - val__perplexity: 4357.1526\n",
      "Epoch 5/10\n",
      "154596/154596 [==============================] - 83s 540us/step - loss: 4.0496 - _perplexity: 364.8819 - val_loss: 5.5017 - val__perplexity: 4483.3748\n",
      "Epoch 6/10\n",
      "154596/154596 [==============================] - 82s 533us/step - loss: 4.0055 - _perplexity: 358.6735 - val_loss: 5.5116 - val__perplexity: 4318.6139\n",
      "Epoch 7/10\n",
      "154596/154596 [==============================] - 82s 530us/step - loss: 3.9735 - _perplexity: 352.6021 - val_loss: 5.5357 - val__perplexity: 4467.1468\n",
      "Epoch 8/10\n",
      "154596/154596 [==============================] - 82s 531us/step - loss: 3.9485 - _perplexity: 350.1786 - val_loss: 5.5409 - val__perplexity: 4423.4720\n",
      "Epoch 9/10\n",
      "154596/154596 [==============================] - 82s 529us/step - loss: 3.9277 - _perplexity: 348.5014 - val_loss: 5.5641 - val__perplexity: 4512.3077\n",
      "Epoch 10/10\n",
      "154596/154596 [==============================] - 82s 530us/step - loss: 3.9111 - _perplexity: 347.1675 - val_loss: 5.5640 - val__perplexity: 4508.9173\n"
     ]
    }
   ],
   "source": [
    "model.train(X,y, epochs=10, batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'deep within within within within within within within within'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict('Deep', 9)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dlinto] *",
   "language": "python",
   "name": "conda-env-dlinto-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
