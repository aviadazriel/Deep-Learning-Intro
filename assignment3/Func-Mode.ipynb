{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.layers import Embedding, CuDNNLSTM, Bidirectional, Dense, CuDNNGRU, LSTM, GRU, Dropout, Concatenate, Input, Flatten\n",
    "from keras.layers import *\n",
    "from keras.initializers import Constant\n",
    "from keras import Sequential\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "from keras.backend import epsilon\n",
    "import ipdb\n",
    "import random\n",
    "from keras_layer_normalization import LayerNormalization\n",
    "from keras import regularizers\n",
    "from keras.models import Model\n",
    "\n",
    "\n",
    "EMBEDDING_DIM = 300\n",
    "INPUT_LENGTH = 1\n",
    "MELODY_VEC_LENGTH = 150\n",
    "\n",
    "class ModelFoo:\n",
    "    def __init__(self, tokenizer, embedding_matrix,\n",
    "                 rnn_units=50,\n",
    "                 bidirectional=True,\n",
    "                 rnn_type='lstm',\n",
    "                 dropout=0.3,\n",
    "                 show_summary=True):\n",
    "        rnn_types = {\n",
    "            'lstm': CuDNNLSTM,\n",
    "            'gru': CuDNNGRU\n",
    "        }\n",
    "        rnn_type = rnn_types[rnn_type]\n",
    "\n",
    "        # load pre-trained word embeddings into an Embedding layer\n",
    "        # note that we set trainable = False so as to keep the embeddings fixed\n",
    "        num_words = len(tokenizer.word_index) + 1\n",
    "        embedding_layer = Embedding(num_words,\n",
    "                                    EMBEDDING_DIM,\n",
    "                                    embeddings_initializer=Constant(embedding_matrix),\n",
    "                                    input_length=INPUT_LENGTH,\n",
    "                                    trainable=True)\n",
    "        lyrics_input = Input(shape=(INPUT_LENGTH,))\n",
    "        melody_input = Input(shape=(MELODY_VEC_LENGTH,))\n",
    "        \n",
    "#         melody = Flatten()(melody_input)\n",
    "        lyrics = embedding_layer(lyrics_input)\n",
    "        lyrics = Flatten()(lyrics)\n",
    "        lyrics = Dropout(dropout)(lyrics)\n",
    "        combined = Concatenate()([lyrics, melody_input])\n",
    "#         combined = Reshape((1, EMBEDDING_DIM + MELODY_VEC_LENGTH))(combined)\n",
    "        combined = rnn_type(rnn_units)(combined)\n",
    "        if bidirectional:\n",
    "            combined = Bidirectional(combined)\n",
    "#         combined = LayerNormalization()(combined)\n",
    "        combined = Dense(num_words, kernel_regularizer=regularizers.l2(0.1), activation='softmax')(combined)\n",
    "        model = Model(inputs=[lyrics_input, melody_input], outputs=[combined])\n",
    "\n",
    "        if show_summary:\n",
    "            model.summary()\n",
    "\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        \n",
    "    def train(self, X, y, epochs=5, batch_size=32, callbacks=[]):\n",
    "        model = self.model\n",
    "        # compile network\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[_perplexity])\n",
    "        # fit network\n",
    "        model.fit(X, y, \n",
    "                  epochs=epochs,\n",
    "                  batch_size=batch_size,\n",
    "                  verbose=1, \n",
    "                  shuffle=True,\n",
    "                  validation_split=0.1,\n",
    "                  callbacks=callbacks)\n",
    "\n",
    "        \n",
    "    def predict(self, first_word, n_words):\n",
    "        in_text, result = first_word, first_word\n",
    "        # generate a fixed number of words\n",
    "        for _ in range(n_words):\n",
    "            # encode the text as integer\n",
    "            encoded = self.tokenizer.texts_to_sequences([in_text])[0]\n",
    "            encoded = np.array(encoded)\n",
    "            \n",
    "            words_probs = self.model.predict_proba(encoded, verbose=0)[0]\n",
    "            \n",
    "            # get 2 arrays of probs and word_tokens\n",
    "            words_probs_enu = list(enumerate(words_probs))\n",
    "            words_probs_sorted = sorted(words_probs_enu, key=lambda x: x[1], reverse=True) # sorting in descending order\n",
    "            words_tokens, words_probs = list(zip(*words_probs_sorted))\n",
    "            # normalizre to sum 1 \n",
    "            words_probs = np.array(words_probs, dtype=np.float64)\n",
    "            words_probs /= words_probs.sum().astype(np.float64)\n",
    "            word_token = np.random.choice(words_tokens, p=words_probs)\n",
    "            \n",
    "            # map predicted word index to word\n",
    "            out_word = get_word(word_token, self.tokenizer)\n",
    "            # append to input\n",
    "            in_text, result = out_word, result + ' ' + out_word\n",
    "        return result      \n",
    "\n",
    "        \n",
    "    def predict_beam(self, first_word, n_words, B=3):\n",
    "        tokenizer = self.tokenizer\n",
    "        model = self.model\n",
    "        in_text, result = first_word, [first_word]\n",
    "        encoded = get_encoded(in_text, tokenizer)\n",
    "        beam_sequences_scores = [[encoded, 0]]\n",
    "\n",
    "        while len(result) < n_words:\n",
    "            all_candidates = []\n",
    "            beam_sequences_scores = self.beam_step(beam_sequences_scores, B)\n",
    "            for seq_score in beam_sequences_scores:\n",
    "                seq_scores = self.beam_step([seq_score], B)\n",
    "                all_candidates.append(seq_scores)\n",
    "            flatten = lambda lst: [item for sublist in lst for item in sublist]\n",
    "            beam_sequences_scores = sorted(flatten(all_candidates), reverse=True, key=lambda tup: tup[1])[:B]\n",
    "            result, _= beam_sequences_scores[0]\n",
    "        \n",
    "        words = [get_word(token, self.tokenizer) for token in result]\n",
    "        return ' '.join(words)\n",
    "    \n",
    "    \n",
    "    def beam_step(self, beam_sequences_scores, B):            \n",
    "        all_candidates = []\n",
    "        for seq, score in beam_sequences_scores: # for each sequence\n",
    "            # predict top B words\n",
    "            seq_pad = pad_sequences([[sample] for sample in seq], maxlen=INPUT_LENGTH)\n",
    "            words_probs = self.model.predict_proba(seq_pad, verbose=0)[0]\n",
    "            words_probs_enu = list(enumerate(words_probs))\n",
    "            words_probs_sorted = sorted(words_probs_enu, key=lambda x: x[1], reverse=True) # sorting in descending order\n",
    "            top_b_words_probs = words_probs_sorted[:B] # top B words with max probability\n",
    "            # for each prob in top B words, create a candidate\n",
    "            for token, prob in top_b_words_probs: \n",
    "                word_token = token\n",
    "                candidate = [np.append(seq, word_token), score + np.log(prob + epsilon())] # todo: word_token \n",
    "                all_candidates.append(candidate)\n",
    "        # take candidates with max score\n",
    "        beam_sequences_scores = sorted(all_candidates, reverse=True, key=lambda tup: tup[1])[:B]\n",
    "        return beam_sequences_scores\n",
    "\n",
    "    \n",
    "def get_encoded(text, tokenizer):\n",
    "    encoded = tokenizer.texts_to_sequences([text])[0]\n",
    "    encoded = np.array(encoded)\n",
    "    return encoded\n",
    "\n",
    "def get_word(index, tokenizer):\n",
    "    for word, idx in tokenizer.word_index.items():\n",
    "         if idx == index:\n",
    "            return word\n",
    "\n",
    "def _perplexity(y_true, y_pred):\n",
    "    cross_entropy = K.categorical_crossentropy(y_true, y_pred)\n",
    "    perplexity = K.pow(2.0, cross_entropy)\n",
    "    return perplexity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.dataset as ds\n",
    "from src.embeddings import extract_embedding_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 614/614 [00:01<00:00, 361.92it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 614/614 [00:01<00:00, 360.66it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 614/614 [00:01<00:00, 352.40it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 614/614 [00:01<00:00, 352.40it/s]\n"
     ]
    }
   ],
   "source": [
    "_, _, tokenizer = ds.load_tokenized_data()\n",
    "embedding_matrix = extract_embedding_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_33 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_17 (Embedding)        (None, 1, 300)       2247300     input_33[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_13 (Flatten)            (None, 300)          0           embedding_17[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_16 (Dropout)            (None, 300)          0           flatten_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "input_34 (InputLayer)           (None, 150)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_15 (Concatenate)    (None, 450)          0           dropout_16[0][0]                 \n",
      "                                                                 input_34[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_4 (Reshape)             (None, 1, 450)       0           concatenate_15[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "cu_dnngru_11 (CuDNNGRU)         (None, 2)            2724        reshape_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 7491)         22473       cu_dnngru_11[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 2,272,497\n",
      "Trainable params: 2,272,497\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1 = ModelFoo(tokenizer, embedding_matrix, rnn_type='gru', rnn_units=2, bidirectional=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "X, y, songs = joblib.load(\"x_y_songs.jblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(190277,)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(190277, 1, 150)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "songs.reshape(-1,1,MELODY_VEC_LENGTH).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected dense_4 to have shape (7491,) but got array with shape (1,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-67-adad6f24b8e9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msongs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-61-227bc8e531a9>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, X, y, epochs, batch_size, callbacks)\u001b[0m\n\u001b[0;32m     72\u001b[0m                   \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m                   \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m                   callbacks=callbacks)\n\u001b[0m\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dlinto\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 952\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m    953\u001b[0m         \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dlinto\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    787\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 789\u001b[1;33m                 exception_prefix='target')\n\u001b[0m\u001b[0;32m    790\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m             \u001b[1;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dlinto\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    136\u001b[0m                             \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' but got array with shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 138\u001b[1;33m                             str(data_shape))\n\u001b[0m\u001b[0;32m    139\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking target: expected dense_4 to have shape (7491,) but got array with shape (1,)"
     ]
    }
   ],
   "source": [
    "model1.train([X,songs], y, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "outFoo = model1.model.predict(np_X_res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
