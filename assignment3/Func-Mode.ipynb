{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.layers import Embedding, CuDNNLSTM, Bidirectional, Dense, CuDNNGRU, LSTM, GRU, Dropout, Concatenate, Input, Flatten\n",
    "from keras.layers import *\n",
    "from keras.initializers import Constant\n",
    "from keras import Sequential\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "from keras.backend import epsilon\n",
    "import ipdb\n",
    "import random\n",
    "from keras_layer_normalization import LayerNormalization\n",
    "from keras import regularizers\n",
    "from keras.models import Model\n",
    "\n",
    "\n",
    "EMBEDDING_DIM = 300\n",
    "INPUT_LENGTH = 1\n",
    "MELODY_VEC_LENGTH = 150\n",
    "\n",
    "class ModelFoo:\n",
    "    def __init__(self, tokenizer, embedding_matrix,\n",
    "                 rnn_units=50,\n",
    "                 bidirectional=True,\n",
    "                 rnn_type='lstm',\n",
    "                 dropout=0.3,\n",
    "                 show_summary=True):\n",
    "        rnn_types = {\n",
    "            'lstm': CuDNNLSTM,\n",
    "            'gru': CuDNNGRU\n",
    "        }\n",
    "        rnn_type = rnn_types[rnn_type]\n",
    "\n",
    "        # load pre-trained word embeddings into an Embedding layer\n",
    "        # note that we set trainable = False so as to keep the embeddings fixed\n",
    "        num_words = len(tokenizer.word_index) + 1\n",
    "        embedding_layer = Embedding(num_words,\n",
    "                                    EMBEDDING_DIM,\n",
    "                                    embeddings_initializer=Constant(embedding_matrix),\n",
    "                                    input_length=INPUT_LENGTH,\n",
    "                                    trainable=True)\n",
    "        lyrics_input = Input(shape=(INPUT_LENGTH,))\n",
    "        melody_input = Input(shape=(MELODY_VEC_LENGTH,))\n",
    "        \n",
    "#         melody = Flatten()(melody_input)\n",
    "        lyrics = embedding_layer(lyrics_input)\n",
    "        lyrics = Flatten()(lyrics)\n",
    "        lyrics = Dropout(dropout)(lyrics)\n",
    "        combined = Concatenate()([lyrics, melody_input])\n",
    "        combined = Reshape((1, EMBEDDING_DIM + MELODY_VEC_LENGTH))(combined)\n",
    "        combined = rnn_type(rnn_units)(combined)\n",
    "        if bidirectional:\n",
    "            combined = Bidirectional(combined)\n",
    "#         combined = LayerNormalization()(combined)\n",
    "        combined = Dense(num_words, kernel_regularizer=regularizers.l2(0.1), activation='softmax')(combined)\n",
    "        model = Model(inputs=[lyrics_input, melody_input], outputs=[combined])\n",
    "\n",
    "        if show_summary:\n",
    "            model.summary()\n",
    "\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        \n",
    "    def train(self, X, y, epochs=5, batch_size=32, callbacks=[]):\n",
    "        model = self.model\n",
    "        # compile network\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[_perplexity])\n",
    "        # fit network\n",
    "        model.fit(X, y, \n",
    "                  epochs=epochs,\n",
    "                  batch_size=batch_size,\n",
    "                  verbose=1, \n",
    "                  shuffle=True,\n",
    "                  validation_split=0.1,\n",
    "                  callbacks=callbacks)\n",
    "\n",
    "        \n",
    "    def predict(self, first_word, n_words):\n",
    "        in_text, result = first_word, first_word\n",
    "        # generate a fixed number of words\n",
    "        for _ in range(n_words):\n",
    "            # encode the text as integer\n",
    "            encoded = self.tokenizer.texts_to_sequences([in_text])[0]\n",
    "            encoded = np.array(encoded)\n",
    "            \n",
    "            words_probs = self.model.predict_proba(encoded, verbose=0)[0]\n",
    "            \n",
    "            # get 2 arrays of probs and word_tokens\n",
    "            words_probs_enu = list(enumerate(words_probs))\n",
    "            words_probs_sorted = sorted(words_probs_enu, key=lambda x: x[1], reverse=True) # sorting in descending order\n",
    "            words_tokens, words_probs = list(zip(*words_probs_sorted))\n",
    "            # normalizre to sum 1 \n",
    "            words_probs = np.array(words_probs, dtype=np.float64)\n",
    "            words_probs /= words_probs.sum().astype(np.float64)\n",
    "            word_token = np.random.choice(words_tokens, p=words_probs)\n",
    "            \n",
    "            # map predicted word index to word\n",
    "            out_word = get_word(word_token, self.tokenizer)\n",
    "            # append to input\n",
    "            in_text, result = out_word, result + ' ' + out_word\n",
    "        return result      \n",
    "\n",
    "        \n",
    "    def predict_beam(self, first_word, n_words, B=3):\n",
    "        tokenizer = self.tokenizer\n",
    "        model = self.model\n",
    "        in_text, result = first_word, [first_word]\n",
    "        encoded = get_encoded(in_text, tokenizer)\n",
    "        beam_sequences_scores = [[encoded, 0]]\n",
    "\n",
    "        while len(result) < n_words:\n",
    "            all_candidates = []\n",
    "            beam_sequences_scores = self.beam_step(beam_sequences_scores, B)\n",
    "            for seq_score in beam_sequences_scores:\n",
    "                seq_scores = self.beam_step([seq_score], B)\n",
    "                all_candidates.append(seq_scores)\n",
    "            flatten = lambda lst: [item for sublist in lst for item in sublist]\n",
    "            beam_sequences_scores = sorted(flatten(all_candidates), reverse=True, key=lambda tup: tup[1])[:B]\n",
    "            result, _= beam_sequences_scores[0]\n",
    "        \n",
    "        words = [get_word(token, self.tokenizer) for token in result]\n",
    "        return ' '.join(words)\n",
    "    \n",
    "    \n",
    "    def beam_step(self, beam_sequences_scores, B):            \n",
    "        all_candidates = []\n",
    "        for seq, score in beam_sequences_scores: # for each sequence\n",
    "            # predict top B words\n",
    "            seq_pad = pad_sequences([[sample] for sample in seq], maxlen=INPUT_LENGTH)\n",
    "            words_probs = self.model.predict_proba(seq_pad, verbose=0)[0]\n",
    "            words_probs_enu = list(enumerate(words_probs))\n",
    "            words_probs_sorted = sorted(words_probs_enu, key=lambda x: x[1], reverse=True) # sorting in descending order\n",
    "            top_b_words_probs = words_probs_sorted[:B] # top B words with max probability\n",
    "            # for each prob in top B words, create a candidate\n",
    "            for token, prob in top_b_words_probs: \n",
    "                word_token = token\n",
    "                candidate = [np.append(seq, word_token), score + np.log(prob + epsilon())] # todo: word_token \n",
    "                all_candidates.append(candidate)\n",
    "        # take candidates with max score\n",
    "        beam_sequences_scores = sorted(all_candidates, reverse=True, key=lambda tup: tup[1])[:B]\n",
    "        return beam_sequences_scores\n",
    "\n",
    "    \n",
    "def get_encoded(text, tokenizer):\n",
    "    encoded = tokenizer.texts_to_sequences([text])[0]\n",
    "    encoded = np.array(encoded)\n",
    "    return encoded\n",
    "\n",
    "def get_word(index, tokenizer):\n",
    "    for word, idx in tokenizer.word_index.items():\n",
    "         if idx == index:\n",
    "            return word\n",
    "\n",
    "def _perplexity(y_true, y_pred):\n",
    "    cross_entropy = K.categorical_crossentropy(y_true, y_pred)\n",
    "    perplexity = K.pow(2.0, cross_entropy)\n",
    "    return perplexity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.dataset as ds\n",
    "from src.embeddings import extract_embedding_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 614/614 [00:01<00:00, 364.79it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 614/614 [00:01<00:00, 362.82it/s]\n"
     ]
    }
   ],
   "source": [
    "x, y, tokenizer, songs = joblib.load(\"x_y_tokenizer_songs.jblib\")\n",
    "embedding_matrix = extract_embedding_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 1, 300)       2247300     input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 300)          0           embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 300)          0           flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            (None, 150)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 450)          0           dropout_3[0][0]                  \n",
      "                                                                 input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_2 (Reshape)             (None, 1, 450)       0           concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "cu_dnngru_3 (CuDNNGRU)          (None, 50)           75300       reshape_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 7491)         382041      cu_dnngru_3[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 2,704,641\n",
      "Trainable params: 2,704,641\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1 = ModelFoo(tokenizer, embedding_matrix, rnn_type='gru', rnn_units=50, bidirectional=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 171249 samples, validate on 19028 samples\n",
      "Epoch 1/10\n",
      "171249/171249 [==============================] - 28s 163us/step - loss: 6.6572 - _perplexity: 195.9328 - val_loss: 6.4176 - val__perplexity: 316.2075\n",
      "Epoch 2/10\n",
      "171249/171249 [==============================] - 26s 151us/step - loss: 5.8648 - _perplexity: 167.8010 - val_loss: 6.2426 - val__perplexity: 939.6688\n",
      "Epoch 3/10\n",
      "171249/171249 [==============================] - 26s 151us/step - loss: 5.6493 - _perplexity: 192.1527 - val_loss: 6.2647 - val__perplexity: 3407.3534\n",
      "Epoch 4/10\n",
      "171249/171249 [==============================] - 26s 151us/step - loss: 5.5690 - _perplexity: 218.1963 - val_loss: 6.2647 - val__perplexity: 4207.7273\n",
      "Epoch 5/10\n",
      "171249/171249 [==============================] - 26s 153us/step - loss: 5.5361 - _perplexity: 239.3797 - val_loss: 6.2525 - val__perplexity: 4233.2446\n",
      "Epoch 6/10\n",
      "171249/171249 [==============================] - 26s 153us/step - loss: 5.5208 - _perplexity: 256.2517 - val_loss: 6.2404 - val__perplexity: 4247.2406\n",
      "Epoch 7/10\n",
      "171249/171249 [==============================] - 26s 154us/step - loss: 5.5121 - _perplexity: 269.0040 - val_loss: 6.2535 - val__perplexity: 4271.5039\n",
      "Epoch 8/10\n",
      "171249/171249 [==============================] - 26s 153us/step - loss: 5.5057 - _perplexity: 279.6491 - val_loss: 6.2549 - val__perplexity: 4286.9831\n",
      "Epoch 9/10\n",
      "171249/171249 [==============================] - 26s 153us/step - loss: 5.5014 - _perplexity: 288.7164 - val_loss: 6.2529 - val__perplexity: 4298.3142\n",
      "Epoch 10/10\n",
      "171249/171249 [==============================] - 26s 152us/step - loss: 5.4985 - _perplexity: 296.3073 - val_loss: 6.2649 - val__perplexity: 4306.2170\n"
     ]
    }
   ],
   "source": [
    "model1.train([x,songs], y, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mSignature:\u001b[0m \u001b[0mmodel1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfirst_word\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m <no docstring>\n",
       "\u001b[1;31mFile:\u001b[0m      c:\\users\\eli\\workspace\\deep-learning-intro\\assignment3\\<ipython-input-12-227bc8e531a9>\n",
       "\u001b[1;31mType:\u001b[0m      method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?model1.predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Model' object has no attribute 'predict_proba'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-4dafbb1ed759>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfirst_word\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'deep'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-12-227bc8e531a9>\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, first_word, n_words)\u001b[0m\n\u001b[0;32m     83\u001b[0m             \u001b[0mencoded\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoded\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m             \u001b[0mwords_probs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoded\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m             \u001b[1;31m# get 2 arrays of probs and word_tokens\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Model' object has no attribute 'predict_proba'"
     ]
    }
   ],
   "source": [
    "model1.predict(first_word='deep', n_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "songs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.3026547e-09, 1.2094216e-06, 7.8628642e-07, ..., 9.0679328e-07,\n",
       "        1.2554902e-06, 1.1776601e-03]], dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.model.predict([[5], [songs[0]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
