{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Todo\n",
    "1. use [center loss](https://medium.com/mlreview/experiments-with-a-new-loss-term-added-to-the-standard-cross-entropy-85b080c42446)\n",
    "2. split train & val\n",
    "3. add [Beam Search](https://machinelearningmastery.com/beam-search-decoder-natural-language-processing/)\n",
    "4. maybe add [attention](https://towardsdatascience.com/light-on-math-ml-attention-with-keras-dc8dbc1fad39)\n",
    "5. add [reg](https://machinelearningmastery.com/how-to-reduce-overfitting-with-dropout-regularization-in-keras/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import src.dataset as ds\n",
    "import numpy as np\n",
    "from src.embeddings import extract_embedding_weights\n",
    "from keras.layers import Embedding, CuDNNLSTM, Bidirectional, Dense, CuDNNGRU\n",
    "from keras.initializers import Constant\n",
    "from keras import Sequential\n",
    "from tensorflow.python.keras.callbacks import TensorBoard\n",
    "from src.model import Model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from time import time\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = tf.ConfigProto()\n",
    "# config.gpu_options.allow_growth = True\n",
    "# tf.keras.backend.set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "tensorboard = TensorBoard(log_dir='./logs', histogram_freq=0,\n",
    "                          write_graph=True, write_images=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████| 615/615 [00:01<00:00, 361.03it/s]\n",
      "100%|███████████████████████████████| 615/615 [00:01<00:00, 370.13it/s]\n",
      "100%|███████████████████████████████| 615/615 [00:01<00:00, 369.23it/s]\n",
      "100%|███████████████████████████████| 615/615 [00:01<00:00, 365.74it/s]\n"
     ]
    }
   ],
   "source": [
    "X, y, tokenizer = ds.load_tokenized_data()\n",
    "embedding_matrix = extract_embedding_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 1, 300)            2251800   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1, 300)            0         \n",
      "_________________________________________________________________\n",
      "cu_dnngru_2 (CuDNNGRU)       (None, 100)               120600    \n",
      "_________________________________________________________________\n",
      "layer_normalization_2 (Layer (None, 100)               200       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 7506)              758106    \n",
      "=================================================================\n",
      "Total params: 3,130,706\n",
      "Trainable params: 3,130,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 173921 samples, validate on 19325 samples\n",
      "Epoch 1/1000\n",
      "173921/173921 [==============================] - 20s 117us/step - loss: 6.1036 - _perplexity: 206.4499 - val_loss: 5.9892 - val__perplexity: 312.4251\n",
      "Epoch 2/1000\n",
      "173921/173921 [==============================] - 17s 99us/step - loss: 5.3486 - _perplexity: 181.4721 - val_loss: 5.8782 - val__perplexity: 477.7044\n",
      "Epoch 3/1000\n",
      "173921/173921 [==============================] - 17s 99us/step - loss: 5.1538 - _perplexity: 181.1689 - val_loss: 5.9078 - val__perplexity: 809.7813\n",
      "Epoch 4/1000\n",
      "173921/173921 [==============================] - 17s 99us/step - loss: 5.0588 - _perplexity: 184.1768 - val_loss: 5.9514 - val__perplexity: 1533.6968\n",
      "Epoch 5/1000\n",
      "173921/173921 [==============================] - 17s 99us/step - loss: 5.0026 - _perplexity: 186.5854 - val_loss: 6.0100 - val__perplexity: 2755.2313\n",
      "Epoch 6/1000\n",
      "173921/173921 [==============================] - 17s 99us/step - loss: 4.9610 - _perplexity: 189.7363 - val_loss: 6.0583 - val__perplexity: 3734.8769\n",
      "Epoch 7/1000\n",
      "173921/173921 [==============================] - 17s 99us/step - loss: 4.9329 - _perplexity: 192.5602 - val_loss: 6.0660 - val__perplexity: 4085.6554\n",
      "Epoch 8/1000\n",
      "173921/173921 [==============================] - 17s 99us/step - loss: 4.9101 - _perplexity: 195.1969 - val_loss: 6.0668 - val__perplexity: 4102.8340\n",
      "Epoch 9/1000\n",
      "173921/173921 [==============================] - 17s 100us/step - loss: 4.8904 - _perplexity: 197.2672 - val_loss: 6.0611 - val__perplexity: 4122.4964\n",
      "Epoch 10/1000\n",
      "173921/173921 [==============================] - 17s 100us/step - loss: 4.8763 - _perplexity: 199.2155 - val_loss: 6.0794 - val__perplexity: 4112.7630\n",
      "Epoch 11/1000\n",
      "173921/173921 [==============================] - 17s 100us/step - loss: 4.8663 - _perplexity: 201.1529 - val_loss: 6.0718 - val__perplexity: 4145.0734\n",
      "Epoch 12/1000\n",
      "173921/173921 [==============================] - 17s 100us/step - loss: 4.8552 - _perplexity: 203.6039 - val_loss: 6.0573 - val__perplexity: 4133.0623\n",
      "Epoch 13/1000\n",
      "173921/173921 [==============================] - 17s 99us/step - loss: 4.8458 - _perplexity: 205.5799 - val_loss: 6.0572 - val__perplexity: 4146.8711\n",
      "Epoch 14/1000\n",
      "173921/173921 [==============================] - 17s 99us/step - loss: 4.8376 - _perplexity: 206.7212 - val_loss: 6.0954 - val__perplexity: 4153.0030\n",
      "Epoch 15/1000\n",
      "173921/173921 [==============================] - 17s 100us/step - loss: 4.8342 - _perplexity: 209.9220 - val_loss: 6.0736 - val__perplexity: 4158.4542\n",
      "Epoch 16/1000\n",
      "173921/173921 [==============================] - 17s 99us/step - loss: 4.8237 - _perplexity: 210.7440 - val_loss: 6.0707 - val__perplexity: 4173.1030\n",
      "Epoch 17/1000\n",
      "173921/173921 [==============================] - 17s 99us/step - loss: 4.8197 - _perplexity: 212.2341 - val_loss: 6.0815 - val__perplexity: 4180.4500\n",
      "Epoch 18/1000\n",
      "173921/173921 [==============================] - 17s 99us/step - loss: 4.8146 - _perplexity: 213.3653 - val_loss: 6.0825 - val__perplexity: 4192.5193\n",
      "Epoch 19/1000\n",
      "173921/173921 [==============================] - 17s 99us/step - loss: 4.8105 - _perplexity: 214.0144 - val_loss: 6.0889 - val__perplexity: 4181.0880\n",
      "Epoch 20/1000\n",
      "173921/173921 [==============================] - 17s 100us/step - loss: 4.8085 - _perplexity: 215.1701 - val_loss: 6.0958 - val__perplexity: 4179.5204\n",
      "Epoch 21/1000\n",
      "173921/173921 [==============================] - 17s 100us/step - loss: 4.8042 - _perplexity: 216.4828 - val_loss: 6.0652 - val__perplexity: 4202.9705\n",
      "Epoch 22/1000\n",
      "173921/173921 [==============================] - 17s 100us/step - loss: 4.8011 - _perplexity: 218.1721 - val_loss: 6.0945 - val__perplexity: 4194.9707\n",
      "Epoch 23/1000\n",
      "173921/173921 [==============================] - 17s 99us/step - loss: 4.7962 - _perplexity: 219.3923 - val_loss: 6.1136 - val__perplexity: 4222.0748\n",
      "Epoch 24/1000\n",
      "173921/173921 [==============================] - 17s 100us/step - loss: 4.7922 - _perplexity: 219.9297 - val_loss: 6.0643 - val__perplexity: 4180.6689\n",
      "Epoch 25/1000\n",
      "173921/173921 [==============================] - 17s 100us/step - loss: 4.7909 - _perplexity: 220.6837 - val_loss: 6.0907 - val__perplexity: 4218.1913\n",
      "Epoch 26/1000\n",
      "173921/173921 [==============================] - 17s 100us/step - loss: 4.7889 - _perplexity: 222.9603 - val_loss: 6.0873 - val__perplexity: 4208.4166\n",
      "Epoch 27/1000\n",
      "173921/173921 [==============================] - 17s 100us/step - loss: 4.7877 - _perplexity: 223.0419 - val_loss: 6.1008 - val__perplexity: 4280.5616\n",
      "Epoch 28/1000\n",
      "173921/173921 [==============================] - 17s 100us/step - loss: 4.7844 - _perplexity: 223.7054 - val_loss: 6.0986 - val__perplexity: 4246.5791\n",
      "Epoch 29/1000\n",
      "173921/173921 [==============================] - 17s 101us/step - loss: 4.7821 - _perplexity: 224.8168 - val_loss: 6.0643 - val__perplexity: 4240.5097\n",
      "Epoch 30/1000\n",
      "173921/173921 [==============================] - 17s 101us/step - loss: 4.7813 - _perplexity: 225.3055 - val_loss: 6.0864 - val__perplexity: 4233.1006\n",
      "Epoch 31/1000\n",
      "173921/173921 [==============================] - 17s 101us/step - loss: 4.7781 - _perplexity: 226.7142 - val_loss: 6.0726 - val__perplexity: 4217.7849\n",
      "Epoch 32/1000\n",
      "173921/173921 [==============================] - 17s 101us/step - loss: 4.7765 - _perplexity: 227.0381 - val_loss: 6.0958 - val__perplexity: 4258.4462\n",
      "Epoch 33/1000\n",
      "173921/173921 [==============================] - 18s 101us/step - loss: 4.7754 - _perplexity: 227.7796 - val_loss: 6.0704 - val__perplexity: 4231.1815\n",
      "Epoch 34/1000\n",
      "173921/173921 [==============================] - 18s 101us/step - loss: 4.7741 - _perplexity: 228.9027 - val_loss: 6.1015 - val__perplexity: 4262.1774\n",
      "Epoch 35/1000\n",
      "173921/173921 [==============================] - 18s 101us/step - loss: 4.7723 - _perplexity: 229.3280 - val_loss: 6.0967 - val__perplexity: 4234.8611\n",
      "Epoch 36/1000\n",
      "173921/173921 [==============================] - 17s 99us/step - loss: 4.7700 - _perplexity: 230.0666 - val_loss: 6.0931 - val__perplexity: 4281.3026\n",
      "Epoch 37/1000\n",
      "173921/173921 [==============================] - 17s 98us/step - loss: 4.7704 - _perplexity: 231.0035 - val_loss: 6.0856 - val__perplexity: 4288.0514\n",
      "Epoch 38/1000\n",
      "173921/173921 [==============================] - 17s 99us/step - loss: 4.7696 - _perplexity: 231.0436 - val_loss: 6.0794 - val__perplexity: 4266.6158\n",
      "Epoch 39/1000\n",
      "173921/173921 [==============================] - 17s 98us/step - loss: 4.7663 - _perplexity: 231.2865 - val_loss: 6.0918 - val__perplexity: 4316.4032\n",
      "Epoch 40/1000\n",
      "173921/173921 [==============================] - 17s 98us/step - loss: 4.7673 - _perplexity: 232.2675 - val_loss: 6.0865 - val__perplexity: 4273.2815\n",
      "Epoch 41/1000\n",
      "173921/173921 [==============================] - 17s 99us/step - loss: 4.7660 - _perplexity: 232.3357 - val_loss: 6.1001 - val__perplexity: 4280.6208\n",
      "Epoch 42/1000\n",
      "173921/173921 [==============================] - 17s 98us/step - loss: 4.7658 - _perplexity: 232.8792 - val_loss: 6.0994 - val__perplexity: 4298.1884\n",
      "Epoch 43/1000\n",
      "173921/173921 [==============================] - 17s 98us/step - loss: 4.7649 - _perplexity: 233.9277 - val_loss: 6.0918 - val__perplexity: 4281.4970\n",
      "Epoch 44/1000\n",
      "173921/173921 [==============================] - 17s 99us/step - loss: 4.7631 - _perplexity: 233.4994 - val_loss: 6.0979 - val__perplexity: 4296.2709\n",
      "Epoch 45/1000\n",
      "173921/173921 [==============================] - 17s 98us/step - loss: 4.7631 - _perplexity: 234.1111 - val_loss: 6.0789 - val__perplexity: 4278.8766\n",
      "Epoch 46/1000\n",
      "173921/173921 [==============================] - 17s 98us/step - loss: 4.7621 - _perplexity: 235.0753 - val_loss: 6.0699 - val__perplexity: 4300.2123\n",
      "Epoch 47/1000\n",
      "173921/173921 [==============================] - 17s 98us/step - loss: 4.7604 - _perplexity: 235.8670 - val_loss: 6.1285 - val__perplexity: 4350.3979\n",
      "Epoch 48/1000\n",
      "173921/173921 [==============================] - 17s 99us/step - loss: 4.7592 - _perplexity: 235.4136 - val_loss: 6.1041 - val__perplexity: 4297.8893\n",
      "Epoch 49/1000\n",
      "173921/173921 [==============================] - 17s 98us/step - loss: 4.7603 - _perplexity: 236.1633 - val_loss: 6.0848 - val__perplexity: 4325.7818\n",
      "Epoch 50/1000\n",
      "173921/173921 [==============================] - 17s 99us/step - loss: 4.7575 - _perplexity: 235.0353 - val_loss: 6.1244 - val__perplexity: 4344.9238\n",
      "Epoch 51/1000\n",
      "173921/173921 [==============================] - 17s 99us/step - loss: 4.7586 - _perplexity: 237.3014 - val_loss: 6.1217 - val__perplexity: 4370.6857\n",
      "Epoch 52/1000\n",
      "173921/173921 [==============================] - 17s 99us/step - loss: 4.7577 - _perplexity: 236.6318 - val_loss: 6.1285 - val__perplexity: 4411.5325\n",
      "Epoch 53/1000\n",
      "173921/173921 [==============================] - 17s 99us/step - loss: 4.7564 - _perplexity: 237.3043 - val_loss: 6.1167 - val__perplexity: 4314.4733\n",
      "Epoch 54/1000\n",
      "173921/173921 [==============================] - 17s 99us/step - loss: 4.7561 - _perplexity: 237.3712 - val_loss: 6.1092 - val__perplexity: 4325.7971\n",
      "Epoch 55/1000\n",
      "173921/173921 [==============================] - 17s 99us/step - loss: 4.7558 - _perplexity: 237.3909 - val_loss: 6.1109 - val__perplexity: 4364.2820\n",
      "Epoch 56/1000\n",
      "173921/173921 [==============================] - 17s 99us/step - loss: 4.7540 - _perplexity: 237.2772 - val_loss: 6.1179 - val__perplexity: 4335.8623\n",
      "Epoch 57/1000\n",
      "173921/173921 [==============================] - 17s 99us/step - loss: 4.7548 - _perplexity: 239.6188 - val_loss: 6.0820 - val__perplexity: 4312.5482\n",
      "Epoch 58/1000\n",
      "173921/173921 [==============================] - 17s 99us/step - loss: 4.7543 - _perplexity: 236.9392 - val_loss: 6.1416 - val__perplexity: 4374.4840\n",
      "Epoch 59/1000\n",
      "173921/173921 [==============================] - 17s 99us/step - loss: 4.7536 - _perplexity: 239.0303 - val_loss: 6.1025 - val__perplexity: 4348.5289\n",
      "Epoch 60/1000\n",
      "173921/173921 [==============================] - 17s 99us/step - loss: 4.7537 - _perplexity: 239.3904 - val_loss: 6.1135 - val__perplexity: 4367.8688\n",
      "Epoch 61/1000\n",
      "173921/173921 [==============================] - 17s 99us/step - loss: 4.7534 - _perplexity: 239.1033 - val_loss: 6.1276 - val__perplexity: 4394.4265\n",
      "Epoch 62/1000\n",
      "173921/173921 [==============================] - 17s 99us/step - loss: 4.7525 - _perplexity: 240.4499 - val_loss: 6.1528 - val__perplexity: 4386.9465\n",
      "Epoch 63/1000\n",
      "173921/173921 [==============================] - 17s 99us/step - loss: 4.7514 - _perplexity: 238.6148 - val_loss: 6.1309 - val__perplexity: 4362.1129\n",
      "Epoch 64/1000\n",
      "173921/173921 [==============================] - 17s 100us/step - loss: 4.7505 - _perplexity: 239.7978 - val_loss: 6.1109 - val__perplexity: 4389.3889\n",
      "Epoch 65/1000\n",
      "173921/173921 [==============================] - 17s 99us/step - loss: 4.7506 - _perplexity: 239.6139 - val_loss: 6.1055 - val__perplexity: 4367.6044\n",
      "Epoch 66/1000\n",
      "173921/173921 [==============================] - 17s 99us/step - loss: 4.7497 - _perplexity: 240.5043 - val_loss: 6.1471 - val__perplexity: 4426.0184\n",
      "Epoch 67/1000\n",
      "173921/173921 [==============================] - 17s 99us/step - loss: 4.7504 - _perplexity: 240.1911 - val_loss: 6.1401 - val__perplexity: 4391.6552\n",
      "Epoch 68/1000\n",
      "173921/173921 [==============================] - 17s 99us/step - loss: 4.7494 - _perplexity: 240.9773 - val_loss: 6.1405 - val__perplexity: 4398.2285\n",
      "Epoch 69/1000\n",
      "173921/173921 [==============================] - 17s 100us/step - loss: 4.7487 - _perplexity: 242.4244 - val_loss: 6.1350 - val__perplexity: 4433.0942\n",
      "Epoch 70/1000\n",
      "173921/173921 [==============================] - 17s 99us/step - loss: 4.7480 - _perplexity: 240.9253 - val_loss: 6.1518 - val__perplexity: 4418.4209\n",
      "Epoch 71/1000\n",
      "173921/173921 [==============================] - 17s 99us/step - loss: 4.7471 - _perplexity: 240.8457 - val_loss: 6.1138 - val__perplexity: 4361.5177\n",
      "Epoch 72/1000\n",
      "173921/173921 [==============================] - 17s 99us/step - loss: 4.7472 - _perplexity: 241.6066 - val_loss: 6.1392 - val__perplexity: 4355.5692\n",
      "Epoch 73/1000\n",
      "173921/173921 [==============================] - 17s 100us/step - loss: 4.7483 - _perplexity: 241.0802 - val_loss: 6.1172 - val__perplexity: 4392.7664\n",
      "Epoch 74/1000\n",
      "173921/173921 [==============================] - 17s 99us/step - loss: 4.7475 - _perplexity: 241.1619 - val_loss: 6.1453 - val__perplexity: 4394.4966\n",
      "Epoch 75/1000\n",
      "173921/173921 [==============================] - 17s 100us/step - loss: 4.7456 - _perplexity: 240.8394 - val_loss: 6.1571 - val__perplexity: 4437.7913\n",
      "Epoch 76/1000\n",
      "173921/173921 [==============================] - 17s 99us/step - loss: 4.7446 - _perplexity: 242.0940 - val_loss: 6.1391 - val__perplexity: 4411.6169\n",
      "Epoch 77/1000\n",
      "173921/173921 [==============================] - 17s 99us/step - loss: 4.7455 - _perplexity: 241.9955 - val_loss: 6.1470 - val__perplexity: 4428.8380\n",
      "Epoch 78/1000\n",
      "173921/173921 [==============================] - 17s 100us/step - loss: 4.7450 - _perplexity: 242.1854 - val_loss: 6.1400 - val__perplexity: 4476.7246\n",
      "Epoch 79/1000\n",
      "173921/173921 [==============================] - 17s 100us/step - loss: 4.7460 - _perplexity: 243.1673 - val_loss: 6.1227 - val__perplexity: 4466.1994\n",
      "Epoch 80/1000\n",
      "173921/173921 [==============================] - 17s 101us/step - loss: 4.7455 - _perplexity: 242.8624 - val_loss: 6.1460 - val__perplexity: 4420.3059\n",
      "Epoch 81/1000\n",
      "173921/173921 [==============================] - 17s 100us/step - loss: 4.7447 - _perplexity: 241.7339 - val_loss: 6.1115 - val__perplexity: 4420.7199\n",
      "Epoch 82/1000\n",
      "173921/173921 [==============================] - 17s 100us/step - loss: 4.7435 - _perplexity: 242.4966 - val_loss: 6.1215 - val__perplexity: 4412.5238\n",
      "Epoch 83/1000\n",
      "173921/173921 [==============================] - 17s 100us/step - loss: 4.7437 - _perplexity: 242.8739 - val_loss: 6.1273 - val__perplexity: 4405.1164\n",
      "Epoch 84/1000\n",
      "173921/173921 [==============================] - 17s 100us/step - loss: 4.7445 - _perplexity: 242.7349 - val_loss: 6.1212 - val__perplexity: 4419.9075\n",
      "Epoch 85/1000\n",
      "173921/173921 [==============================] - 17s 100us/step - loss: 4.7432 - _perplexity: 242.4041 - val_loss: 6.1383 - val__perplexity: 4443.9115\n",
      "Epoch 86/1000\n",
      "173921/173921 [==============================] - 17s 100us/step - loss: 4.7426 - _perplexity: 242.9385 - val_loss: 6.1228 - val__perplexity: 4383.7314\n",
      "Epoch 87/1000\n",
      "173921/173921 [==============================] - 17s 100us/step - loss: 4.7425 - _perplexity: 243.3009 - val_loss: 6.1338 - val__perplexity: 4467.0767\n",
      "Epoch 88/1000\n",
      "173921/173921 [==============================] - 17s 100us/step - loss: 4.7441 - _perplexity: 243.3607 - val_loss: 6.1146 - val__perplexity: 4379.6202\n",
      "Epoch 89/1000\n",
      "173921/173921 [==============================] - 17s 100us/step - loss: 4.7431 - _perplexity: 242.5309 - val_loss: 6.1596 - val__perplexity: 4456.3941\n",
      "Epoch 90/1000\n",
      "173921/173921 [==============================] - 17s 100us/step - loss: 4.7433 - _perplexity: 243.5919 - val_loss: 6.1203 - val__perplexity: 4439.5869\n",
      "Epoch 91/1000\n",
      "173921/173921 [==============================] - 17s 100us/step - loss: 4.7414 - _perplexity: 244.1908 - val_loss: 6.1397 - val__perplexity: 4478.2344\n",
      "Epoch 92/1000\n",
      "173921/173921 [==============================] - 17s 100us/step - loss: 4.7427 - _perplexity: 244.1657 - val_loss: 6.1256 - val__perplexity: 4394.5004\n",
      "Epoch 93/1000\n",
      "173921/173921 [==============================] - 17s 100us/step - loss: 4.7404 - _perplexity: 244.7317 - val_loss: 6.1218 - val__perplexity: 4448.2652\n",
      "Epoch 94/1000\n",
      "173921/173921 [==============================] - 17s 100us/step - loss: 4.7400 - _perplexity: 243.8221 - val_loss: 6.1516 - val__perplexity: 4530.1609\n",
      "Epoch 95/1000\n",
      "173921/173921 [==============================] - 17s 99us/step - loss: 4.7426 - _perplexity: 244.1399 - val_loss: 6.1357 - val__perplexity: 4421.4970\n",
      "Epoch 96/1000\n",
      "173921/173921 [==============================] - 17s 100us/step - loss: 4.7391 - _perplexity: 243.0944 - val_loss: 6.1205 - val__perplexity: 4468.0384\n",
      "Epoch 97/1000\n",
      "173921/173921 [==============================] - 17s 100us/step - loss: 4.7382 - _perplexity: 243.9903 - val_loss: 6.1439 - val__perplexity: 4426.9489\n",
      "Epoch 98/1000\n",
      "173921/173921 [==============================] - 17s 99us/step - loss: 4.7418 - _perplexity: 242.9805 - val_loss: 6.1446 - val__perplexity: 4529.4795\n",
      "Epoch 99/1000\n",
      "173921/173921 [==============================] - 17s 100us/step - loss: 4.7402 - _perplexity: 244.9053 - val_loss: 6.1368 - val__perplexity: 4410.9742\n",
      "Epoch 100/1000\n",
      "173921/173921 [==============================] - 17s 100us/step - loss: 4.7393 - _perplexity: 243.5619 - val_loss: 6.1508 - val__perplexity: 4473.6484\n",
      "Epoch 101/1000\n",
      "173921/173921 [==============================] - 17s 99us/step - loss: 4.7412 - _perplexity: 243.0995 - val_loss: 6.1099 - val__perplexity: 4487.9349\n",
      "Epoch 102/1000\n",
      "173921/173921 [==============================] - 17s 100us/step - loss: 4.7402 - _perplexity: 243.7340 - val_loss: 6.1225 - val__perplexity: 4435.4899\n",
      "Epoch 103/1000\n",
      "173921/173921 [==============================] - 17s 99us/step - loss: 4.7399 - _perplexity: 243.7081 - val_loss: 6.1463 - val__perplexity: 4479.1269\n",
      "Epoch 104/1000\n",
      "173921/173921 [==============================] - 17s 100us/step - loss: 4.7388 - _perplexity: 244.8019 - val_loss: 6.1444 - val__perplexity: 4475.6377\n",
      "Epoch 105/1000\n",
      "173921/173921 [==============================] - 17s 99us/step - loss: 4.7371 - _perplexity: 242.9067 - val_loss: 6.1547 - val__perplexity: 4458.0891\n",
      "Epoch 106/1000\n",
      "173921/173921 [==============================] - 17s 100us/step - loss: 4.7397 - _perplexity: 243.5026 - val_loss: 6.1424 - val__perplexity: 4465.9571\n",
      "Epoch 107/1000\n",
      "173921/173921 [==============================] - 17s 100us/step - loss: 4.7401 - _perplexity: 243.4026 - val_loss: 6.1315 - val__perplexity: 4425.7952\n",
      "Epoch 108/1000\n",
      "173921/173921 [==============================] - 17s 100us/step - loss: 4.7395 - _perplexity: 243.8282 - val_loss: 6.1434 - val__perplexity: 4458.2858\n",
      "Epoch 109/1000\n",
      "173921/173921 [==============================] - 17s 99us/step - loss: 4.7387 - _perplexity: 243.3071 - val_loss: 6.1399 - val__perplexity: 4494.8650\n",
      "Epoch 110/1000\n",
      "173921/173921 [==============================] - 17s 99us/step - loss: 4.7378 - _perplexity: 245.0802 - val_loss: 6.1654 - val__perplexity: 4522.6689\n",
      "Epoch 111/1000\n",
      "173921/173921 [==============================] - 17s 99us/step - loss: 4.7389 - _perplexity: 243.6269 - val_loss: 6.1268 - val__perplexity: 4473.5424\n",
      "Epoch 112/1000\n",
      "173921/173921 [==============================] - 17s 99us/step - loss: 4.7378 - _perplexity: 243.8628 - val_loss: 6.1322 - val__perplexity: 4517.5272\n",
      "Epoch 113/1000\n",
      "173921/173921 [==============================] - 17s 100us/step - loss: 4.7372 - _perplexity: 243.3264 - val_loss: 6.1379 - val__perplexity: 4459.5939\n",
      "Epoch 114/1000\n",
      "173921/173921 [==============================] - 17s 100us/step - loss: 4.7377 - _perplexity: 243.9592 - val_loss: 6.1501 - val__perplexity: 4491.5233\n",
      "Epoch 115/1000\n",
      "173921/173921 [==============================] - 17s 100us/step - loss: 4.7364 - _perplexity: 244.5798 - val_loss: 6.1318 - val__perplexity: 4469.4914\n",
      "Epoch 116/1000\n",
      "173921/173921 [==============================] - 17s 100us/step - loss: 4.7365 - _perplexity: 245.4091 - val_loss: 6.1446 - val__perplexity: 4462.5161\n",
      "Epoch 117/1000\n",
      "173921/173921 [==============================] - 17s 99us/step - loss: 4.7370 - _perplexity: 243.8848 - val_loss: 6.1326 - val__perplexity: 4472.9875\n",
      "Epoch 118/1000\n",
      "173921/173921 [==============================] - 17s 99us/step - loss: 4.7363 - _perplexity: 244.9937 - val_loss: 6.1347 - val__perplexity: 4475.7412\n",
      "Epoch 119/1000\n",
      "173921/173921 [==============================] - 17s 100us/step - loss: 4.7366 - _perplexity: 244.0265 - val_loss: 6.1305 - val__perplexity: 4478.4364\n",
      "Epoch 120/1000\n",
      "173921/173921 [==============================] - 17s 100us/step - loss: 4.7352 - _perplexity: 245.3787 - val_loss: 6.1260 - val__perplexity: 4478.4233\n",
      "Epoch 121/1000\n",
      "173921/173921 [==============================] - 17s 100us/step - loss: 4.7349 - _perplexity: 243.6857 - val_loss: 6.1525 - val__perplexity: 4521.8983\n",
      "Epoch 122/1000\n",
      "173921/173921 [==============================] - 17s 100us/step - loss: 4.7356 - _perplexity: 244.2909 - val_loss: 6.1399 - val__perplexity: 4523.2148\n",
      "Epoch 123/1000\n",
      "173921/173921 [==============================] - 17s 99us/step - loss: 4.7351 - _perplexity: 246.4904 - val_loss: 6.1322 - val__perplexity: 4546.6996\n",
      "Epoch 124/1000\n",
      "173921/173921 [==============================] - 17s 99us/step - loss: 4.7371 - _perplexity: 244.5345 - val_loss: 6.1387 - val__perplexity: 4516.2959\n",
      "Epoch 125/1000\n",
      "173921/173921 [==============================] - 17s 99us/step - loss: 4.7356 - _perplexity: 245.4614 - val_loss: 6.1042 - val__perplexity: 4454.3547\n",
      "Epoch 126/1000\n",
      "173921/173921 [==============================] - 17s 100us/step - loss: 4.7356 - _perplexity: 243.7371 - val_loss: 6.1228 - val__perplexity: 4496.3753\n",
      "Epoch 127/1000\n",
      "173921/173921 [==============================] - 17s 99us/step - loss: 4.7350 - _perplexity: 243.2892 - val_loss: 6.1318 - val__perplexity: 4516.8170\n",
      "Epoch 128/1000\n",
      "173921/173921 [==============================] - 17s 100us/step - loss: 4.7353 - _perplexity: 244.2382 - val_loss: 6.1272 - val__perplexity: 4509.1157\n",
      "Epoch 129/1000\n",
      "173921/173921 [==============================] - 17s 99us/step - loss: 4.7346 - _perplexity: 243.3978 - val_loss: 6.1519 - val__perplexity: 4625.1405\n",
      "Epoch 130/1000\n",
      "173921/173921 [==============================] - 17s 99us/step - loss: 4.7333 - _perplexity: 245.4345 - val_loss: 6.1119 - val__perplexity: 4522.6310\n",
      "Epoch 131/1000\n",
      "173921/173921 [==============================] - 17s 100us/step - loss: 4.7334 - _perplexity: 243.8572 - val_loss: 6.1350 - val__perplexity: 4481.5256\n",
      "Epoch 132/1000\n",
      "173921/173921 [==============================] - 17s 100us/step - loss: 4.7348 - _perplexity: 244.0616 - val_loss: 6.1559 - val__perplexity: 4591.9766\n",
      "Epoch 133/1000\n",
      "173921/173921 [==============================] - 17s 99us/step - loss: 4.7361 - _perplexity: 244.5288 - val_loss: 6.1383 - val__perplexity: 4523.9126\n",
      "Epoch 134/1000\n",
      "173921/173921 [==============================] - 17s 100us/step - loss: 4.7338 - _perplexity: 244.9480 - val_loss: 6.1578 - val__perplexity: 4597.6174\n",
      "Epoch 135/1000\n",
      "173921/173921 [==============================] - 17s 100us/step - loss: 4.7360 - _perplexity: 245.2191 - val_loss: 6.1264 - val__perplexity: 4567.9136\n",
      "Epoch 136/1000\n",
      "173921/173921 [==============================] - 17s 100us/step - loss: 4.7337 - _perplexity: 243.9926 - val_loss: 6.1297 - val__perplexity: 4456.5998\n",
      "Epoch 137/1000\n",
      "173921/173921 [==============================] - 17s 101us/step - loss: 4.7340 - _perplexity: 245.1733 - val_loss: 6.1477 - val__perplexity: 4540.6001\n",
      "Epoch 138/1000\n",
      "173921/173921 [==============================] - 17s 99us/step - loss: 4.7338 - _perplexity: 243.6098 - val_loss: 6.1308 - val__perplexity: 4466.6191\n",
      "Epoch 139/1000\n",
      "173921/173921 [==============================] - 17s 99us/step - loss: 4.7338 - _perplexity: 245.0603 - val_loss: 6.1455 - val__perplexity: 4518.0773\n",
      "Epoch 140/1000\n",
      "173921/173921 [==============================] - 17s 99us/step - loss: 4.7324 - _perplexity: 244.5077 - val_loss: 6.1338 - val__perplexity: 4537.6250\n",
      "Epoch 141/1000\n",
      "173921/173921 [==============================] - 17s 100us/step - loss: 4.7324 - _perplexity: 243.6046 - val_loss: 6.1315 - val__perplexity: 4495.7974\n",
      "Epoch 142/1000\n",
      "173921/173921 [==============================] - 17s 99us/step - loss: 4.7315 - _perplexity: 245.6895 - val_loss: 6.1630 - val__perplexity: 4547.9665\n",
      "Epoch 143/1000\n",
      "173921/173921 [==============================] - 17s 99us/step - loss: 4.7321 - _perplexity: 245.1051 - val_loss: 6.1348 - val__perplexity: 4514.2011\n",
      "Epoch 144/1000\n",
      "173921/173921 [==============================] - 17s 99us/step - loss: 4.7318 - _perplexity: 243.6430 - val_loss: 6.1461 - val__perplexity: 4530.2829\n",
      "Epoch 145/1000\n",
      "173921/173921 [==============================] - 17s 100us/step - loss: 4.7337 - _perplexity: 244.6447 - val_loss: 6.1423 - val__perplexity: 4494.5365\n",
      "Epoch 146/1000\n",
      "173921/173921 [==============================] - 17s 99us/step - loss: 4.7343 - _perplexity: 244.8667 - val_loss: 6.1240 - val__perplexity: 4521.0033\n",
      "Epoch 147/1000\n",
      "173921/173921 [==============================] - 17s 99us/step - loss: 4.7313 - _perplexity: 244.6798 - val_loss: 6.1472 - val__perplexity: 4490.1550\n",
      "Epoch 148/1000\n",
      "173921/173921 [==============================] - 17s 99us/step - loss: 4.7329 - _perplexity: 244.3921 - val_loss: 6.1249 - val__perplexity: 4533.8966\n",
      "Epoch 149/1000\n",
      "173921/173921 [==============================] - 17s 100us/step - loss: 4.7326 - _perplexity: 245.5619 - val_loss: 6.1490 - val__perplexity: 4586.4851\n",
      "Epoch 150/1000\n",
      "173921/173921 [==============================] - 17s 99us/step - loss: 4.7319 - _perplexity: 245.1517 - val_loss: 6.1406 - val__perplexity: 4515.0705\n",
      "Epoch 151/1000\n",
      "173921/173921 [==============================] - 17s 99us/step - loss: 4.7303 - _perplexity: 244.3381 - val_loss: 6.1392 - val__perplexity: 4509.9062\n",
      "Epoch 152/1000\n",
      "173921/173921 [==============================] - 17s 99us/step - loss: 4.7314 - _perplexity: 244.1877 - val_loss: 6.1271 - val__perplexity: 4487.6317\n",
      "Epoch 153/1000\n",
      "173921/173921 [==============================] - 17s 99us/step - loss: 4.7314 - _perplexity: 244.4463 - val_loss: 6.1429 - val__perplexity: 4569.5536\n",
      "Epoch 154/1000\n",
      "173921/173921 [==============================] - 17s 99us/step - loss: 4.7314 - _perplexity: 244.5643 - val_loss: 6.1361 - val__perplexity: 4570.2751\n",
      "Epoch 155/1000\n",
      "173921/173921 [==============================] - 17s 99us/step - loss: 4.7306 - _perplexity: 244.9796 - val_loss: 6.1470 - val__perplexity: 4537.7197\n",
      "Epoch 156/1000\n",
      "173921/173921 [==============================] - 17s 99us/step - loss: 4.7315 - _perplexity: 244.7519 - val_loss: 6.1551 - val__perplexity: 4614.3330\n",
      "Epoch 157/1000\n",
      "173921/173921 [==============================] - 17s 99us/step - loss: 4.7320 - _perplexity: 245.2776 - val_loss: 6.1443 - val__perplexity: 4519.3098\n",
      "Epoch 158/1000\n",
      "173921/173921 [==============================] - 17s 99us/step - loss: 4.7318 - _perplexity: 245.9414 - val_loss: 6.1018 - val__perplexity: 4446.7815\n",
      "Epoch 159/1000\n",
      "173921/173921 [==============================] - 17s 99us/step - loss: 4.7313 - _perplexity: 243.5232 - val_loss: 6.1299 - val__perplexity: 4483.7493\n",
      "Epoch 160/1000\n",
      "173921/173921 [==============================] - 17s 99us/step - loss: 4.7308 - _perplexity: 243.5226 - val_loss: 6.1361 - val__perplexity: 4560.7511\n",
      "Epoch 161/1000\n",
      "173921/173921 [==============================] - 17s 99us/step - loss: 4.7316 - _perplexity: 244.4159 - val_loss: 6.1437 - val__perplexity: 4564.6872\n",
      "Epoch 162/1000\n",
      "173921/173921 [==============================] - 17s 99us/step - loss: 4.7308 - _perplexity: 244.6236 - val_loss: 6.1383 - val__perplexity: 4478.7153\n",
      "Epoch 163/1000\n",
      "173921/173921 [==============================] - 17s 99us/step - loss: 4.7312 - _perplexity: 244.5469 - val_loss: 6.1215 - val__perplexity: 4531.0657\n",
      "Epoch 164/1000\n",
      "173921/173921 [==============================] - 17s 99us/step - loss: 4.7307 - _perplexity: 244.7075 - val_loss: 6.1306 - val__perplexity: 4513.6736\n",
      "Epoch 165/1000\n",
      "173921/173921 [==============================] - 18s 102us/step - loss: 4.7319 - _perplexity: 244.5142 - val_loss: 6.1343 - val__perplexity: 4510.7714\n",
      "Epoch 166/1000\n",
      "173921/173921 [==============================] - 17s 100us/step - loss: 4.7299 - _perplexity: 244.2063 - val_loss: 6.1454 - val__perplexity: 4553.1395\n",
      "Epoch 167/1000\n",
      "173921/173921 [==============================] - 17s 101us/step - loss: 4.7311 - _perplexity: 243.7421 - val_loss: 6.1504 - val__perplexity: 4615.4455\n",
      "Epoch 168/1000\n",
      "173921/173921 [==============================] - 18s 102us/step - loss: 4.7298 - _perplexity: 245.8720 - val_loss: 6.1298 - val__perplexity: 4566.4425\n",
      "Epoch 169/1000\n",
      "173921/173921 [==============================] - 18s 102us/step - loss: 4.7299 - _perplexity: 246.2460 - val_loss: 6.1345 - val__perplexity: 4513.4359\n",
      "Epoch 170/1000\n",
      "173921/173921 [==============================] - 17s 100us/step - loss: 4.7287 - _perplexity: 245.4121 - val_loss: 6.1520 - val__perplexity: 4551.2980\n",
      "Epoch 171/1000\n",
      "173921/173921 [==============================] - 19s 108us/step - loss: 4.7296 - _perplexity: 245.6220 - val_loss: 6.1307 - val__perplexity: 4564.2659\n",
      "Epoch 172/1000\n",
      "173921/173921 [==============================] - 18s 102us/step - loss: 4.7282 - _perplexity: 244.5664 - val_loss: 6.1441 - val__perplexity: 4528.5812\n",
      "Epoch 173/1000\n",
      "173921/173921 [==============================] - 17s 101us/step - loss: 4.7291 - _perplexity: 243.8447 - val_loss: 6.1294 - val__perplexity: 4556.1826\n",
      "Epoch 174/1000\n",
      "173921/173921 [==============================] - 17s 100us/step - loss: 4.7306 - _perplexity: 245.5043 - val_loss: 6.1463 - val__perplexity: 4544.3729\n",
      "Epoch 175/1000\n",
      "173921/173921 [==============================] - 17s 100us/step - loss: 4.7288 - _perplexity: 245.1286 - val_loss: 6.1261 - val__perplexity: 4483.0074\n",
      "Epoch 176/1000\n",
      "173921/173921 [==============================] - 17s 100us/step - loss: 4.7294 - _perplexity: 244.6507 - val_loss: 6.1375 - val__perplexity: 4618.1291\n",
      "Epoch 177/1000\n",
      "173921/173921 [==============================] - 17s 100us/step - loss: 4.7294 - _perplexity: 244.5351 - val_loss: 6.1260 - val__perplexity: 4513.2448\n",
      "Epoch 178/1000\n",
      "173921/173921 [==============================] - 17s 100us/step - loss: 4.7297 - _perplexity: 246.3634 - val_loss: 6.1535 - val__perplexity: 4582.8759\n",
      "Epoch 179/1000\n",
      "173921/173921 [==============================] - 17s 100us/step - loss: 4.7282 - _perplexity: 245.5063 - val_loss: 6.1371 - val__perplexity: 4586.2716\n",
      "Epoch 180/1000\n",
      "173921/173921 [==============================] - 17s 100us/step - loss: 4.7289 - _perplexity: 245.3619 - val_loss: 6.1143 - val__perplexity: 4464.3178\n",
      "Epoch 181/1000\n",
      "173921/173921 [==============================] - 17s 100us/step - loss: 4.7292 - _perplexity: 244.9371 - val_loss: 6.1246 - val__perplexity: 4510.2628\n",
      "Epoch 182/1000\n",
      "173921/173921 [==============================] - 17s 100us/step - loss: 4.7274 - _perplexity: 244.7771 - val_loss: 6.1120 - val__perplexity: 4505.7666\n",
      "Epoch 183/1000\n",
      "173921/173921 [==============================] - 17s 100us/step - loss: 4.7298 - _perplexity: 244.3627 - val_loss: 6.1335 - val__perplexity: 4525.2416\n",
      "Epoch 184/1000\n",
      "173921/173921 [==============================] - 18s 101us/step - loss: 4.7308 - _perplexity: 244.9576 - val_loss: 6.1147 - val__perplexity: 4492.8783\n",
      "Epoch 185/1000\n",
      "173921/173921 [==============================] - 18s 101us/step - loss: 4.7280 - _perplexity: 244.6571 - val_loss: 6.1231 - val__perplexity: 4508.2479\n",
      "Epoch 186/1000\n",
      "173921/173921 [==============================] - 18s 103us/step - loss: 4.7292 - _perplexity: 244.6753 - val_loss: 6.1349 - val__perplexity: 4592.3076\n",
      "Epoch 187/1000\n",
      "173921/173921 [==============================] - 18s 103us/step - loss: 4.7282 - _perplexity: 246.4644 - val_loss: 6.1119 - val__perplexity: 4529.5000\n",
      "Epoch 188/1000\n",
      "173921/173921 [==============================] - 17s 100us/step - loss: 4.7296 - _perplexity: 245.0439 - val_loss: 6.1616 - val__perplexity: 4588.3787\n",
      "Epoch 189/1000\n",
      "173921/173921 [==============================] - 19s 111us/step - loss: 4.7276 - _perplexity: 245.7480 - val_loss: 6.1279 - val__perplexity: 4518.9081\n",
      "Epoch 190/1000\n",
      "173921/173921 [==============================] - 18s 101us/step - loss: 4.7302 - _perplexity: 246.0718 - val_loss: 6.1016 - val__perplexity: 4595.9244\n",
      "Epoch 191/1000\n",
      "173921/173921 [==============================] - 17s 100us/step - loss: 4.7267 - _perplexity: 244.8749 - val_loss: 6.1378 - val__perplexity: 4590.7123\n",
      "Epoch 192/1000\n",
      "173921/173921 [==============================] - 17s 100us/step - loss: 4.7283 - _perplexity: 244.4922 - val_loss: 6.1284 - val__perplexity: 4518.7300\n",
      "Epoch 193/1000\n",
      "173921/173921 [==============================] - 17s 99us/step - loss: 4.7286 - _perplexity: 244.3779 - val_loss: 6.1546 - val__perplexity: 4556.0184\n",
      "Epoch 194/1000\n",
      "173921/173921 [==============================] - 17s 100us/step - loss: 4.7282 - _perplexity: 244.7843 - val_loss: 6.1489 - val__perplexity: 4574.9749\n",
      "Epoch 195/1000\n",
      "173921/173921 [==============================] - 17s 99us/step - loss: 4.7283 - _perplexity: 245.4017 - val_loss: 6.1238 - val__perplexity: 4577.5479\n",
      "Epoch 196/1000\n",
      "173921/173921 [==============================] - 17s 99us/step - loss: 4.7272 - _perplexity: 245.2690 - val_loss: 6.1315 - val__perplexity: 4567.4151\n",
      "Epoch 197/1000\n",
      "173921/173921 [==============================] - 17s 100us/step - loss: 4.7271 - _perplexity: 245.5097 - val_loss: 6.0964 - val__perplexity: 4490.6896\n",
      "Epoch 198/1000\n",
      "173921/173921 [==============================] - 17s 99us/step - loss: 4.7276 - _perplexity: 245.1135 - val_loss: 6.1302 - val__perplexity: 4619.1410\n",
      "Epoch 199/1000\n",
      "173921/173921 [==============================] - 17s 99us/step - loss: 4.7267 - _perplexity: 245.4432 - val_loss: 6.1216 - val__perplexity: 4505.0652\n",
      "Epoch 200/1000\n",
      "173921/173921 [==============================] - 17s 100us/step - loss: 4.7275 - _perplexity: 245.6920 - val_loss: 6.1590 - val__perplexity: 4538.4486\n",
      "Epoch 201/1000\n",
      "173921/173921 [==============================] - 18s 103us/step - loss: 4.7277 - _perplexity: 244.5622 - val_loss: 6.1717 - val__perplexity: 4651.6576\n",
      "Epoch 202/1000\n",
      "173921/173921 [==============================] - 18s 102us/step - loss: 4.7280 - _perplexity: 246.7327 - val_loss: 6.1132 - val__perplexity: 4516.4860\n",
      "Epoch 203/1000\n",
      "173921/173921 [==============================] - 17s 100us/step - loss: 4.7271 - _perplexity: 244.3640 - val_loss: 6.1646 - val__perplexity: 4611.9494\n",
      "Epoch 204/1000\n",
      "173921/173921 [==============================] - 18s 102us/step - loss: 4.7280 - _perplexity: 245.1900 - val_loss: 6.1457 - val__perplexity: 4572.5286\n",
      "Epoch 205/1000\n",
      "173921/173921 [==============================] - 17s 99us/step - loss: 4.7266 - _perplexity: 245.5683 - val_loss: 6.1514 - val__perplexity: 4583.8838\n",
      "Epoch 206/1000\n",
      "173921/173921 [==============================] - 17s 98us/step - loss: 4.7270 - _perplexity: 245.2252 - val_loss: 6.1381 - val__perplexity: 4639.1682\n",
      "Epoch 207/1000\n",
      "173921/173921 [==============================] - 17s 99us/step - loss: 4.7274 - _perplexity: 246.2256 - val_loss: 6.1403 - val__perplexity: 4556.2623\n",
      "Epoch 208/1000\n",
      "173921/173921 [==============================] - 17s 100us/step - loss: 4.7275 - _perplexity: 245.8813 - val_loss: 6.1166 - val__perplexity: 4547.4471\n",
      "Epoch 209/1000\n",
      "173921/173921 [==============================] - 17s 99us/step - loss: 4.7269 - _perplexity: 245.8694 - val_loss: 6.1317 - val__perplexity: 4552.9725\n",
      "Epoch 210/1000\n",
      "173921/173921 [==============================] - 17s 99us/step - loss: 4.7256 - _perplexity: 244.5879 - val_loss: 6.1501 - val__perplexity: 4613.3458\n",
      "Epoch 211/1000\n",
      "173921/173921 [==============================] - 18s 101us/step - loss: 4.7271 - _perplexity: 245.4176 - val_loss: 6.1270 - val__perplexity: 4615.8044\n",
      "Epoch 212/1000\n",
      "173921/173921 [==============================] - 18s 101us/step - loss: 4.7263 - _perplexity: 246.6817 - val_loss: 6.1543 - val__perplexity: 4606.8670\n",
      "Epoch 213/1000\n",
      "173921/173921 [==============================] - 18s 102us/step - loss: 4.7283 - _perplexity: 245.6474 - val_loss: 6.1454 - val__perplexity: 4597.8608\n",
      "Epoch 214/1000\n",
      "173921/173921 [==============================] - 17s 98us/step - loss: 4.7268 - _perplexity: 244.7901 - val_loss: 6.1277 - val__perplexity: 4654.2473\n",
      "Epoch 215/1000\n",
      "173921/173921 [==============================] - 17s 97us/step - loss: 4.7275 - _perplexity: 245.3683 - val_loss: 6.1183 - val__perplexity: 4618.2222\n",
      "Epoch 216/1000\n",
      "173921/173921 [==============================] - 17s 99us/step - loss: 4.7265 - _perplexity: 245.0329 - val_loss: 6.1216 - val__perplexity: 4584.0599\n",
      "Epoch 217/1000\n",
      "173921/173921 [==============================] - 17s 97us/step - loss: 4.7247 - _perplexity: 243.9108 - val_loss: 6.1306 - val__perplexity: 4668.6425\n",
      "Epoch 218/1000\n",
      "173921/173921 [==============================] - 17s 99us/step - loss: 4.7259 - _perplexity: 245.1742 - val_loss: 6.1518 - val__perplexity: 4584.6179\n",
      "Epoch 219/1000\n",
      "173921/173921 [==============================] - 18s 101us/step - loss: 4.7284 - _perplexity: 245.0867 - val_loss: 6.1292 - val__perplexity: 4534.8134\n",
      "Epoch 220/1000\n",
      "173921/173921 [==============================] - 18s 102us/step - loss: 4.7267 - _perplexity: 245.5437 - val_loss: 6.1523 - val__perplexity: 4536.6862\n",
      "Epoch 221/1000\n",
      "173921/173921 [==============================] - 18s 102us/step - loss: 4.7266 - _perplexity: 244.6479 - val_loss: 6.1449 - val__perplexity: 4657.0556\n",
      "Epoch 222/1000\n",
      "173921/173921 [==============================] - 18s 101us/step - loss: 4.7246 - _perplexity: 243.5721 - val_loss: 6.1412 - val__perplexity: 4567.0756\n",
      "Epoch 223/1000\n",
      "173921/173921 [==============================] - 17s 101us/step - loss: 4.7236 - _perplexity: 245.6468 - val_loss: 6.1345 - val__perplexity: 4584.8632\n",
      "Epoch 224/1000\n",
      "173921/173921 [==============================] - 18s 102us/step - loss: 4.7253 - _perplexity: 245.0358 - val_loss: 6.1364 - val__perplexity: 4633.9943\n",
      "Epoch 225/1000\n",
      "173921/173921 [==============================] - 18s 103us/step - loss: 4.7301 - _perplexity: 244.9078 - val_loss: 6.1610 - val__perplexity: 4564.1035\n",
      "Epoch 226/1000\n",
      "173921/173921 [==============================] - 17s 101us/step - loss: 4.7280 - _perplexity: 244.5913 - val_loss: 6.1250 - val__perplexity: 4548.3204\n",
      "Epoch 227/1000\n",
      "173921/173921 [==============================] - 18s 101us/step - loss: 4.7264 - _perplexity: 245.4417 - val_loss: 6.1136 - val__perplexity: 4535.1474\n",
      "Epoch 228/1000\n",
      "173921/173921 [==============================] - 17s 99us/step - loss: 4.7277 - _perplexity: 244.0547 - val_loss: 6.1420 - val__perplexity: 4572.5737\n",
      "Epoch 229/1000\n",
      "173921/173921 [==============================] - 17s 99us/step - loss: 4.7264 - _perplexity: 244.8161 - val_loss: 6.1420 - val__perplexity: 4561.8879\n",
      "Epoch 230/1000\n",
      "173921/173921 [==============================] - 17s 100us/step - loss: 4.7272 - _perplexity: 246.2424 - val_loss: 6.1375 - val__perplexity: 4567.6361\n",
      "Epoch 231/1000\n",
      "173921/173921 [==============================] - 17s 100us/step - loss: 4.7267 - _perplexity: 245.5729 - val_loss: 6.1376 - val__perplexity: 4567.5536\n",
      "Epoch 232/1000\n",
      "173921/173921 [==============================] - 18s 103us/step - loss: 4.7272 - _perplexity: 244.6688 - val_loss: 6.1499 - val__perplexity: 4625.8393\n",
      "Epoch 233/1000\n",
      "173921/173921 [==============================] - 17s 101us/step - loss: 4.7267 - _perplexity: 244.1344 - val_loss: 6.1545 - val__perplexity: 4604.3558\n",
      "Epoch 234/1000\n",
      "173921/173921 [==============================] - 17s 99us/step - loss: 4.7256 - _perplexity: 245.8358 - val_loss: 6.1387 - val__perplexity: 4588.9606\n",
      "Epoch 235/1000\n",
      "173921/173921 [==============================] - 17s 99us/step - loss: 4.7263 - _perplexity: 244.9839 - val_loss: 6.1307 - val__perplexity: 4560.0580\n",
      "Epoch 236/1000\n",
      "173921/173921 [==============================] - 17s 99us/step - loss: 4.7257 - _perplexity: 244.3832 - val_loss: 6.1180 - val__perplexity: 4643.5944\n",
      "Epoch 237/1000\n",
      "173921/173921 [==============================] - 17s 99us/step - loss: 4.7261 - _perplexity: 245.6777 - val_loss: 6.1557 - val__perplexity: 4664.7738\n",
      "Epoch 238/1000\n",
      "173921/173921 [==============================] - 18s 101us/step - loss: 4.7281 - _perplexity: 244.8867 - val_loss: 6.1275 - val__perplexity: 4567.4492\n",
      "Epoch 239/1000\n",
      "173921/173921 [==============================] - 17s 99us/step - loss: 4.7251 - _perplexity: 244.7766 - val_loss: 6.1373 - val__perplexity: 4617.4868\n",
      "Epoch 240/1000\n",
      "173921/173921 [==============================] - 17s 100us/step - loss: 4.7262 - _perplexity: 246.0476 - val_loss: 6.1196 - val__perplexity: 4568.8296\n",
      "Epoch 241/1000\n",
      "173921/173921 [==============================] - 17s 100us/step - loss: 4.7274 - _perplexity: 244.9423 - val_loss: 6.1249 - val__perplexity: 4636.4966\n",
      "Epoch 242/1000\n",
      "173921/173921 [==============================] - 17s 98us/step - loss: 4.7266 - _perplexity: 244.4921 - val_loss: 6.1314 - val__perplexity: 4552.3125\n",
      "Epoch 243/1000\n",
      "173921/173921 [==============================] - 18s 101us/step - loss: 4.7262 - _perplexity: 246.1121 - val_loss: 6.1380 - val__perplexity: 4611.8596\n",
      "Epoch 244/1000\n",
      "173921/173921 [==============================] - 17s 100us/step - loss: 4.7247 - _perplexity: 245.4296 - val_loss: 6.1584 - val__perplexity: 4710.4785\n",
      "Epoch 245/1000\n",
      "173921/173921 [==============================] - 17s 100us/step - loss: 4.7256 - _perplexity: 244.4072 - val_loss: 6.1097 - val__perplexity: 4545.5355\n",
      "Epoch 246/1000\n",
      "173921/173921 [==============================] - 17s 100us/step - loss: 4.7268 - _perplexity: 246.0297 - val_loss: 6.1305 - val__perplexity: 4538.8211\n",
      "Epoch 247/1000\n",
      "173921/173921 [==============================] - 17s 100us/step - loss: 4.7243 - _perplexity: 245.4225 - val_loss: 6.1313 - val__perplexity: 4628.7439\n",
      "Epoch 248/1000\n",
      "173921/173921 [==============================] - 17s 100us/step - loss: 4.7256 - _perplexity: 246.9462 - val_loss: 6.1471 - val__perplexity: 4646.5505\n",
      "Epoch 249/1000\n",
      "173921/173921 [==============================] - 17s 100us/step - loss: 4.7261 - _perplexity: 245.5926 - val_loss: 6.1572 - val__perplexity: 4652.2302\n",
      "Epoch 250/1000\n",
      "173921/173921 [==============================] - 17s 100us/step - loss: 4.7257 - _perplexity: 245.1844 - val_loss: 6.1642 - val__perplexity: 4751.1963\n",
      "Epoch 251/1000\n",
      "173921/173921 [==============================] - 17s 100us/step - loss: 4.7260 - _perplexity: 245.7233 - val_loss: 6.1231 - val__perplexity: 4564.2155\n",
      "Epoch 252/1000\n",
      "173921/173921 [==============================] - 17s 100us/step - loss: 4.7255 - _perplexity: 245.6073 - val_loss: 6.1555 - val__perplexity: 4708.8281\n",
      "Epoch 253/1000\n",
      "173921/173921 [==============================] - 17s 100us/step - loss: 4.7252 - _perplexity: 245.4458 - val_loss: 6.1361 - val__perplexity: 4557.6065\n",
      "Epoch 254/1000\n",
      "173921/173921 [==============================] - 17s 100us/step - loss: 4.7265 - _perplexity: 245.8070 - val_loss: 6.1508 - val__perplexity: 4548.8888\n",
      "Epoch 255/1000\n",
      "173921/173921 [==============================] - 17s 99us/step - loss: 4.7249 - _perplexity: 246.5593 - val_loss: 6.1535 - val__perplexity: 4665.2931\n",
      "Epoch 256/1000\n",
      "173921/173921 [==============================] - 17s 100us/step - loss: 4.7243 - _perplexity: 246.0426 - val_loss: 6.1463 - val__perplexity: 4621.0998\n",
      "Epoch 257/1000\n",
      "173760/173921 [============================>.] - ETA: 0s - loss: 4.7253 - _perplexity: 243.1699"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-5734f9a55eb1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membedding_matrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrnn_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'gru'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrnn_units\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbidirectional\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtensorboard\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Workspace\\Deep-Learning-Intro\\assignment3\\src\\model.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, X, y, epochs, batch_size, callbacks)\u001b[0m\n\u001b[0;32m     63\u001b[0m                   \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m                   \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m                   callbacks=callbacks)\n\u001b[0m\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dlinto\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dlinto\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    210\u001b[0m                         val_outs = test_loop(model, val_f, val_ins,\n\u001b[0;32m    211\u001b[0m                                              \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 212\u001b[1;33m                                              verbose=0)\n\u001b[0m\u001b[0;32m    213\u001b[0m                         \u001b[0mval_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_outs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m                         \u001b[1;31m# Same labels assumed.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dlinto\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mtest_loop\u001b[1;34m(model, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[0;32m    390\u001b[0m                 \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    391\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 392\u001b[1;33m             \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    393\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    394\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dlinto\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dlinto\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dlinto\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model1 = Model(tokenizer, embedding_matrix, rnn_type='gru', rnn_units=100, bidirectional=False)\n",
    "model1.train(X,y, epochs=1000, batch_size=64, callbacks=[tensorboard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\". and life is mine . all . you just a jolly distance . . feel on the moment top of god get it really do n't even have to stay darlin . i 'll find stranger . said house my cat . begun . if where the best as long\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.predict(first_word='.', n_words=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 1, 300)            2251800   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 1, 300)            0         \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_1 (CuDNNLSTM)     (None, 100)               160800    \n",
      "_________________________________________________________________\n",
      "layer_normalization_3 (Layer (None, 100)               200       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 7506)              758106    \n",
      "=================================================================\n",
      "Total params: 3,170,906\n",
      "Trainable params: 3,170,906\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 173921 samples, validate on 19325 samples\n",
      "Epoch 1/1000\n",
      "173921/173921 [==============================] - 19s 112us/step - loss: 6.0895 - _perplexity: 204.3711 - val_loss: 5.9393 - val__perplexity: 307.2895\n",
      "Epoch 2/1000\n",
      "173921/173921 [==============================] - 18s 102us/step - loss: 5.3384 - _perplexity: 180.1646 - val_loss: 5.8681 - val__perplexity: 487.8134\n",
      "Epoch 3/1000\n",
      "173921/173921 [==============================] - 18s 101us/step - loss: 5.1525 - _perplexity: 179.6718 - val_loss: 5.8966 - val__perplexity: 880.1318\n",
      "Epoch 4/1000\n",
      "173921/173921 [==============================] - 17s 100us/step - loss: 5.0645 - _perplexity: 182.3492 - val_loss: 5.9429 - val__perplexity: 1526.7888\n",
      "Epoch 5/1000\n",
      "173921/173921 [==============================] - 18s 101us/step - loss: 5.0085 - _perplexity: 185.0548 - val_loss: 5.9749 - val__perplexity: 2775.5783\n",
      "Epoch 6/1000\n",
      " 92864/173921 [===============>..............] - ETA: 7s - loss: 4.9659 - _perplexity: 186.1130"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-c7f428c92965>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membedding_matrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrnn_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'lstm'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrnn_units\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbidirectional\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtensorboard\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Workspace\\Deep-Learning-Intro\\assignment3\\src\\model.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, X, y, epochs, batch_size, callbacks)\u001b[0m\n\u001b[0;32m     63\u001b[0m                   \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m                   \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m                   callbacks=callbacks)\n\u001b[0m\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dlinto\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dlinto\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dlinto\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dlinto\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dlinto\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model2 = Model(tokenizer, embedding_matrix, rnn_type='lstm', rnn_units=100, bidirectional=False)\n",
    "model2.train(X,y, epochs=1000, batch_size=64, callbacks=[tensorboard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"deep pay all cried i remember just know a avenue . in here . lo after sleep away . and add it 's suppose to you . baby i give into me so i see you 're .44 . try . catch it it more . yeah sweet in the surprise\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.predict(first_word='deep', n_words=50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
