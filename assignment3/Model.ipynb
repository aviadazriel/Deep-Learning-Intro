{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Todo\n",
    "1. use [center loss](https://medium.com/mlreview/experiments-with-a-new-loss-term-added-to-the-standard-cross-entropy-85b080c42446)\n",
    "2. split train & val\n",
    "3. add [Beam Search](https://machinelearningmastery.com/beam-search-decoder-natural-language-processing/)\n",
    "4. maybe add [attention](https://towardsdatascience.com/light-on-math-ml-attention-with-keras-dc8dbc1fad39)\n",
    "5. add [reg](https://machinelearningmastery.com/how-to-reduce-overfitting-with-dropout-regularization-in-keras/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Embedding, CuDNNLSTM, Bidirectional, Dense, CuDNNGRU\n",
    "from keras.initializers import Constant\n",
    "from keras import Sequential\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "from keras.backend import epsilon\n",
    "import ipdb\n",
    "import random\n",
    "\n",
    "EMBEDDING_DIM = 300\n",
    "INPUT_LENGTH = 1\n",
    "\n",
    "\n",
    "class Model:\n",
    "    def __init__(self, tokenizer, embedding_matrix,\n",
    "                 rnn_units=50,\n",
    "                 bidirectional=True,\n",
    "                 rnn_type='lstm',\n",
    "                 show_summary=True):\n",
    "        rnn_types = {\n",
    "            'lstm': CuDNNLSTM,\n",
    "            'gru': CuDNNGRU\n",
    "        }\n",
    "        rnn_type = rnn_types[rnn_type]\n",
    "\n",
    "        # load pre-trained word embeddings into an Embedding layer\n",
    "        # note that we set trainable = False so as to keep the embeddings fixed\n",
    "        num_words = len(tokenizer.word_index) + 1\n",
    "        embedding_layer = Embedding(num_words,\n",
    "                                    EMBEDDING_DIM,\n",
    "                                    embeddings_initializer=Constant(embedding_matrix),\n",
    "                                    input_length=INPUT_LENGTH,\n",
    "                                    trainable=False)\n",
    "        model = Sequential()\n",
    "        model.add(embedding_layer)\n",
    "        if bidirectional:\n",
    "            model.add(Bidirectional(rnn_type(rnn_units)))\n",
    "        else:\n",
    "            model.add(rnn_type(rnn_units))\n",
    "        model.add(Dense(num_words, activation='softmax'))\n",
    "        if show_summary:\n",
    "            model.summary()\n",
    "\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        \n",
    "    def train(self, X, y, epochs=5, batch_size=32, callbacks=[]):\n",
    "        model = self.model\n",
    "        # compile network\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[_perplexity])\n",
    "        # fit network\n",
    "        model.fit(X, y, \n",
    "                  epochs=epochs,\n",
    "                  batch_size=batch_size,\n",
    "                  verbose=1, \n",
    "                  shuffle=True,\n",
    "                  validation_split=0.2,\n",
    "                  callbacks=callbacks)\n",
    "\n",
    "        \n",
    "    def predict(self, first_word, n_words, top_k=10):\n",
    "        in_text, result = first_word, first_word\n",
    "        # generate a fixed number of words\n",
    "        for _ in range(n_words):\n",
    "            # encode the text as integer\n",
    "            encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "            encoded = np.array(encoded)\n",
    "            \n",
    "            words_probs = self.model.predict_proba(encoded, verbose=0)[0]\n",
    "            words_probs_enu = list(enumerate(words_probs))\n",
    "            words_probs_sorted = sorted(words_probs_enu, key=lambda x: x[1], reverse=True) # sorting in descending order\n",
    "            top_k_words_probs = words_probs_sorted[:top_k] # top top_k words with max probability\n",
    "            words_tokens, words_probs = list(zip(*top_k_words_probs))\n",
    "            ipdb.set_trace()\n",
    "            word_token, _ = np.random.choice(words_tokens, p=words_probs)\n",
    "            \n",
    "            # map predicted word index to word\n",
    "            out_word = get_word(word_token, self.tokenizer)\n",
    "            # append to input\n",
    "            in_text, result = out_word, result + ' ' + out_word\n",
    "        return result      \n",
    "\n",
    "        \n",
    "    def predict_beam(self, first_word, n_words, B=3):\n",
    "        tokenizer = self.tokenizer\n",
    "        model = self.model\n",
    "        in_text, result = first_word, [first_word]\n",
    "        encoded = get_encoded(in_text, tokenizer)\n",
    "        beam_sequences_scores = [[encoded, 0]]\n",
    "\n",
    "        while len(result) < n_words:\n",
    "            all_candidates = []\n",
    "            beam_sequences_scores = self.beam_step(beam_sequences_scores, B)\n",
    "            for seq_score in beam_sequences_scores:\n",
    "                seq_scores = self.beam_step([seq_score], B)\n",
    "                all_candidates.append(seq_scores)\n",
    "            flatten = lambda lst: [item for sublist in lst for item in sublist]\n",
    "            beam_sequences_scores = sorted(flatten(all_candidates), reverse=True, key=lambda tup: tup[1])[:B]\n",
    "            result, _= beam_sequences_scores[0]\n",
    "        \n",
    "        words = [get_word(token, self.tokenizer) for token in result]\n",
    "        return ' '.join(words)\n",
    "    \n",
    "    \n",
    "    def beam_step(self, beam_sequences_scores, B):            \n",
    "        all_candidates = []\n",
    "        for seq, score in beam_sequences_scores: # for each sequence\n",
    "            # predict top B words\n",
    "            seq_pad = pad_sequences([[sample] for sample in seq], maxlen=INPUT_LENGTH)\n",
    "            words_probs = self.model.predict_proba(seq_pad, verbose=0)[0]\n",
    "            words_probs_enu = list(enumerate(words_probs))\n",
    "            words_probs_sorted = sorted(words_probs_enu, key=lambda x: x[1], reverse=True) # sorting in descending order\n",
    "            top_b_words_probs = words_probs_sorted[:B] # top B words with max probability\n",
    "            # for each prob in top B words, create a candidate\n",
    "            for token, prob in top_b_words_probs: \n",
    "                word_token = token\n",
    "                candidate = [np.append(seq, word_token), score + np.log(prob + epsilon())] # todo: word_token \n",
    "                all_candidates.append(candidate)\n",
    "        # take candidates with max score\n",
    "        beam_sequences_scores = sorted(all_candidates, reverse=True, key=lambda tup: tup[1])[:B]\n",
    "        return beam_sequences_scores\n",
    "\n",
    "    \n",
    "def get_encoded(text, tokenizer):\n",
    "    encoded = tokenizer.texts_to_sequences([text])[0]\n",
    "    encoded = np.array(encoded)\n",
    "    return encoded\n",
    "\n",
    "def get_word(index, tokenizer):\n",
    "    for word, idx in tokenizer.word_index.items():\n",
    "         if idx == index:\n",
    "            return word\n",
    "\n",
    "def _perplexity(y_true, y_pred):\n",
    "    cross_entropy = K.categorical_crossentropy(y_true, y_pred)\n",
    "    perplexity = K.pow(2.0, cross_entropy)\n",
    "    return perplexity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import src.dataset as ds\n",
    "import numpy as np\n",
    "from src.embeddings import extract_embedding_weights\n",
    "from keras.layers import Embedding, CuDNNLSTM, Bidirectional, Dense, CuDNNGRU\n",
    "from keras.initializers import Constant\n",
    "from keras import Sequential\n",
    "from tensorflow.python.keras.callbacks import TensorBoard\n",
    "# from src.model import Model, INPUT_LENGTH\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from time import time\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "tf.keras.backend.set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "tensorboard = TensorBoard(log_dir='./logs', histogram_freq=0,\n",
    "                          write_graph=True, write_images=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 615/615 [00:01<00:00, 373.27it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 615/615 [00:01<00:00, 373.49it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 615/615 [00:01<00:00, 371.69it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 615/615 [00:01<00:00, 364.88it/s]\n"
     ]
    }
   ],
   "source": [
    "X, y, tokenizer = ds.load_tokenized_data()\n",
    "embedding_matrix = extract_embedding_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = pad_sequences([[sample] for sample in X], maxlen=INPUT_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Eli\\Anaconda3\\envs\\dlinto\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 1, 300)            2251800   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 100)               140800    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 7506)              758106    \n",
      "=================================================================\n",
      "Total params: 3,150,706\n",
      "Trainable params: 898,906\n",
      "Non-trainable params: 2,251,800\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From C:\\Users\\Eli\\Anaconda3\\envs\\dlinto\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 154596 samples, validate on 38650 samples\n",
      "Epoch 1/2\n",
      "154596/154596 [==============================] - 42s 269us/step - loss: 5.1364 - _perplexity: 791.0199 - val_loss: 5.4125 - val__perplexity: 2488.7116\n",
      "Epoch 2/2\n",
      "154596/154596 [==============================] - 39s 250us/step - loss: 4.4858 - _perplexity: 448.0178 - val_loss: 5.4178 - val__perplexity: 3876.4251\n"
     ]
    }
   ],
   "source": [
    "model = Model(tokenizer, embedding_matrix)\n",
    "model.train(X,y, epochs=2, batch_size=20, callbacks=[tensorboard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> <ipython-input-1-45123a38aee0>(76)predict()\n",
      "     75             ipdb.set_trace()\n",
      "---> 76             word_token, _ = np.random.choice(words_tokens, p=words_probs)\n",
      "     77 \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ValueError: probabilities do not sum to 1\n",
      "> <ipython-input-1-45123a38aee0>(76)predict()\n",
      "     75             ipdb.set_trace()\n",
      "---> 76             word_token, _ = np.random.choice(words_tokens, p=words_probs)\n",
      "     77 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.predict(first_word='king', n_words=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
