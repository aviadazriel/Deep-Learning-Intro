{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Embedding, CuDNNLSTM, Bidirectional, Dense, CuDNNGRU\n",
    "from keras.initializers import Constant\n",
    "from keras import Sequential\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "from keras.backend import epsilon\n",
    "\n",
    "EMBEDDING_DIM = 300\n",
    "INPUT_LENGTH = 1000\n",
    "\n",
    "\n",
    "class Model:\n",
    "    def __init__(self, tokenizer, embedding_matrix,\n",
    "                 rnn_units=50,\n",
    "                 bidirectional=True,\n",
    "                 rnn_type='lstm',\n",
    "                 show_summary=True):\n",
    "        rnn_types = {\n",
    "            'lstm': CuDNNLSTM,\n",
    "            'gru': CuDNNGRU\n",
    "        }\n",
    "        rnn_type = rnn_types[rnn_type]\n",
    "\n",
    "        # load pre-trained word embeddings into an Embedding layer\n",
    "        # note that we set trainable = False so as to keep the embeddings fixed\n",
    "        num_words = len(tokenizer.word_index) + 1\n",
    "        embedding_layer = Embedding(num_words,\n",
    "                                    EMBEDDING_DIM,\n",
    "                                    embeddings_initializer=Constant(embedding_matrix),\n",
    "                                    input_length=INPUT_LENGTH,\n",
    "                                    trainable=False)\n",
    "        model = Sequential()\n",
    "        model.add(embedding_layer)\n",
    "        if bidirectional:\n",
    "            model.add(Bidirectional(rnn_type(rnn_units)))\n",
    "        else:\n",
    "            model.add(rnn_type(rnn_units))\n",
    "        model.add(Dense(num_words, activation='softmax'))\n",
    "        if show_summary:\n",
    "            model.summary()\n",
    "\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def train(self, X, y, epochs=5, batch_size=32, callbacks=[]):\n",
    "        model = self.model\n",
    "        # compile network\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[_perplexity])\n",
    "        # fit network\n",
    "        model.fit(X, y, \n",
    "                  epochs=epochs,\n",
    "                  batch_size=batch_size,\n",
    "                  verbose=1, \n",
    "                  shuffle=True,\n",
    "                  validation_split=0.2,\n",
    "                  callbacks=callbacks)\n",
    "\n",
    "    def predict(self, first_word, n_words, B=3):\n",
    "        tokenizer = self.tokenizer\n",
    "        model = self.model\n",
    "        in_text, result = first_word, first_word\n",
    "        encoded = get_encoded(in_text, tokenizer)\n",
    "        beam_sequences_scores = [[[encoded], 0]]\n",
    "\n",
    "        while len(result) < n_words:\n",
    "            all_candidates = []\n",
    "            beam_sequences_scores = beam_step(beam_sequences_scores)\n",
    "            assert len(beam_sequences_scores) == B\n",
    "            for seq_score in beam_sequences_scores:\n",
    "                seq_scores = beam_step(seq_score)\n",
    "                all_candidates.append(seq_scores)\n",
    "            beam_sequences_scores = sorted(all_candidates, reverse=True, key=lambda seq, score: score)[:B]\n",
    "            assert len(beam_sequences_scores) == B\n",
    "            result, _= beam_sequences_scores[0]\n",
    "        \n",
    "        words = [get_word(token) for token in result]\n",
    "        return ' '.join(words)\n",
    "        \n",
    "def beam_step(beam_sequences_scores):            \n",
    "    all_candidates = []\n",
    "    for seq, score in beam_sequences_scores: # for each sequence\n",
    "        # predict top B words\n",
    "        words_probs = model.predict_proba(seq, verbose=0)\n",
    "        words_probs_sorted = -np.sort(-words_probs) # sorting in descending order\n",
    "        top_b_words_probs = words_probs_sorted[:B] # top B words with max probability\n",
    "        # for each prob in top B words, create a candidate\n",
    "        for prob in top_b_words_probs: \n",
    "            word_token = list(top_b_words_probs).index(prob)\n",
    "            candidate = [seq + [word_token], score + np.log(prob + epsilon())] # todo: word_token \n",
    "            all_candidates.append(candidate)\n",
    "    # take candidates with max score\n",
    "    beam_sequences_scores = sorted(all_candidates, reverse=True, key=lambda tup: tup[1])[:B]\n",
    "    return beam_sequences_scores\n",
    "\n",
    "def get_encoded(text, tokenizer):\n",
    "    encoded = self.tokenizer.texts_to_sequences([text])[0]\n",
    "    encoded = np.array(encoded)\n",
    "    return encoded\n",
    "\n",
    "def get_word(index):\n",
    "    for word, idx in self.tokenizer.word_index.items():\n",
    "         if idx == index:\n",
    "            return word\n",
    "\n",
    "def _perplexity(y_true, y_pred):\n",
    "    cross_entropy = K.categorical_crossentropy(y_true, y_pred)\n",
    "    perplexity = K.pow(2.0, cross_entropy)\n",
    "    return perplexity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import src.dataset as ds\n",
    "import numpy as np\n",
    "from src.embeddings import extract_embedding_weights\n",
    "from keras.layers import Embedding, CuDNNLSTM, Bidirectional, Dense, CuDNNGRU\n",
    "from keras.initializers import Constant\n",
    "from keras import Sequential\n",
    "from tensorflow.python.keras.callbacks import TensorBoard\n",
    "# from src.model import Model\n",
    "\n",
    "from time import time\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 615/615 [00:01<00:00, 357.69it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 615/615 [00:01<00:00, 364.44it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 615/615 [00:01<00:00, 364.23it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 615/615 [00:01<00:00, 360.39it/s]\n"
     ]
    }
   ],
   "source": [
    "X, y, tokenizer = ds.load_tokenized_data()\n",
    "embedding_matrix = extract_embedding_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Eli\\Anaconda3\\envs\\dlinto\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 1000, 300)         2251800   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 100)               140800    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 7506)              758106    \n",
      "=================================================================\n",
      "Total params: 3,150,706\n",
      "Trainable params: 898,906\n",
      "Non-trainable params: 2,251,800\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model(tokenizer, embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected embedding_1_input to have shape (1000,) but got array with shape (1,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-2366f82227fc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-3-1a401e366151>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, X, y, epochs, batch_size, callbacks)\u001b[0m\n\u001b[0;32m     54\u001b[0m                   \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m                   \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m                   callbacks=callbacks)\n\u001b[0m\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfirst_word\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mB\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dlinto\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 952\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m    953\u001b[0m         \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dlinto\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    749\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    750\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 751\u001b[1;33m             exception_prefix='input')\n\u001b[0m\u001b[0;32m    752\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    753\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dlinto\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    136\u001b[0m                             \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' but got array with shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 138\u001b[1;33m                             str(data_shape))\n\u001b[0m\u001b[0;32m    139\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking input: expected embedding_1_input to have shape (1000,) but got array with shape (1,)"
     ]
    }
   ],
   "source": [
    "model.train(X,y, epochs=1, batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dlinto] *",
   "language": "python",
   "name": "conda-env-dlinto-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
