{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding, CuDNNLSTM, Bidirectional, Dense, CuDNNGRU\n",
    "from keras.initializers import Constant\n",
    "from keras import Sequential\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "from keras.backend import epsilon\n",
    "import ipdb\n",
    "\n",
    "EMBEDDING_DIM = 300\n",
    "INPUT_LENGTH = 50\n",
    "\n",
    "\n",
    "class Model:\n",
    "    def __init__(self, tokenizer, embedding_matrix,\n",
    "                 rnn_units=50,\n",
    "                 bidirectional=True,\n",
    "                 rnn_type='lstm',\n",
    "                 show_summary=True):\n",
    "        rnn_types = {\n",
    "            'lstm': CuDNNLSTM,\n",
    "            'gru': CuDNNGRU\n",
    "        }\n",
    "        rnn_type = rnn_types[rnn_type]\n",
    "\n",
    "        # load pre-trained word embeddings into an Embedding layer\n",
    "        # note that we set trainable = False so as to keep the embeddings fixed\n",
    "        num_words = len(tokenizer.word_index) + 1\n",
    "        embedding_layer = Embedding(num_words,\n",
    "                                    EMBEDDING_DIM,\n",
    "                                    embeddings_initializer=Constant(embedding_matrix),\n",
    "                                    input_length=INPUT_LENGTH,\n",
    "                                    trainable=False)\n",
    "        model = Sequential()\n",
    "        model.add(embedding_layer)\n",
    "        if bidirectional:\n",
    "            model.add(Bidirectional(rnn_type(rnn_units)))\n",
    "        else:\n",
    "            model.add(rnn_type(rnn_units))\n",
    "        model.add(Dense(num_words, activation='softmax'))\n",
    "        if show_summary:\n",
    "            model.summary()\n",
    "\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def train(self, X, y, epochs=5, batch_size=32, callbacks=[]):\n",
    "        model = self.model\n",
    "        # compile network\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[_perplexity])\n",
    "        # fit network\n",
    "        model.fit(X, y, \n",
    "                  epochs=epochs,\n",
    "                  batch_size=batch_size,\n",
    "                  verbose=1, \n",
    "                  shuffle=True,\n",
    "                  validation_split=0.2,\n",
    "                  callbacks=callbacks)\n",
    "\n",
    "    def predict(self, first_word, n_words, B=3):\n",
    "        tokenizer = self.tokenizer\n",
    "        model = self.model\n",
    "        in_text, result = first_word, [first_word]\n",
    "        encoded = get_encoded(in_text, tokenizer)\n",
    "        beam_sequences_scores = [[encoded, 0]]\n",
    "\n",
    "        while len(result) < n_words:\n",
    "            all_candidates = []\n",
    "            beam_sequences_scores = self.beam_step(beam_sequences_scores, B)\n",
    "            for seq_score in beam_sequences_scores:\n",
    "                seq_scores = self.beam_step([seq_score], B)\n",
    "                all_candidates.append(seq_scores)\n",
    "            flatten = lambda lst: [item for sublist in lst for item in sublist]\n",
    "            beam_sequences_scores = sorted(flatten(all_candidates), reverse=True, key=lambda tup: tup[1])[:B]\n",
    "            result, _= beam_sequences_scores[0]\n",
    "        \n",
    "        words = [get_word(token, self.tokenizer) for token in result]\n",
    "        return ' '.join(words)\n",
    "    \n",
    "    \n",
    "    def beam_step(self, beam_sequences_scores, B):            \n",
    "        all_candidates = []\n",
    "        for seq, score in beam_sequences_scores: # for each sequence\n",
    "            # predict top B words\n",
    "            seq_pad = pad_sequences([[sample] for sample in seq], maxlen=INPUT_LENGTH)\n",
    "            words_probs = self.model.predict_proba(seq_pad, verbose=0)[0]\n",
    "            words_probs_enu = list(enumerate(words_probs))\n",
    "            words_probs_sorted = sorted(words_probs_enu, key=lambda x: x[1], reverse=True) # sorting in descending order\n",
    "            top_b_words_probs = words_probs_sorted[:B] # top B words with max probability\n",
    "            # for each prob in top B words, create a candidate\n",
    "            for token, prob in top_b_words_probs: \n",
    "                word_token = token\n",
    "                candidate = [np.append(seq, word_token), score + np.log(prob + epsilon())] # todo: word_token \n",
    "                all_candidates.append(candidate)\n",
    "        # take candidates with max score\n",
    "        beam_sequences_scores = sorted(all_candidates, reverse=True, key=lambda tup: tup[1])[:B]\n",
    "        return beam_sequences_scores\n",
    "\n",
    "def get_encoded(text, tokenizer):\n",
    "    encoded = tokenizer.texts_to_sequences([text])[0]\n",
    "    encoded = np.array(encoded)\n",
    "    return encoded\n",
    "\n",
    "def get_word(index, tokenizer):\n",
    "    for word, idx in tokenizer.word_index.items():\n",
    "         if idx == index:\n",
    "            return word\n",
    "\n",
    "def _perplexity(y_true, y_pred):\n",
    "    cross_entropy = K.categorical_crossentropy(y_true, y_pred)\n",
    "    perplexity = K.pow(2.0, cross_entropy)\n",
    "    return perplexity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import src.dataset as ds\n",
    "import numpy as np\n",
    "from src.embeddings import extract_embedding_weights\n",
    "from keras.layers import Embedding, CuDNNLSTM, Bidirectional, Dense, CuDNNGRU\n",
    "from keras.initializers import Constant\n",
    "from keras import Sequential\n",
    "from tensorflow.python.keras.callbacks import TensorBoard\n",
    "# from src.model import Model\n",
    "\n",
    "from time import time\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 615/615 [00:01<00:00, 328.52it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 615/615 [00:01<00:00, 331.88it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 615/615 [00:01<00:00, 334.41it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 615/615 [00:01<00:00, 332.42it/s]\n"
     ]
    }
   ],
   "source": [
    "X, y, tokenizer = ds.load_tokenized_data()\n",
    "embedding_matrix = extract_embedding_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "X = pad_sequences([[sample] for sample in X], maxlen=INPUT_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 50, 300)           2251800   \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 300)               406800    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 7506)              2259306   \n",
      "=================================================================\n",
      "Total params: 4,917,906\n",
      "Trainable params: 2,666,106\n",
      "Non-trainable params: 2,251,800\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model(tokenizer, embedding_matrix, rnn_units=150, rnn_type='gru')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 154596 samples, validate on 38650 samples\n",
      "Epoch 1/10\n",
      "154596/154596 [==============================] - 83s 536us/step - loss: 4.4895 - _perplexity: 706.9609 - val_loss: 5.4373 - val__perplexity: 3762.3366\n",
      "Epoch 2/10\n",
      "154596/154596 [==============================] - 82s 532us/step - loss: 4.2389 - _perplexity: 430.1123 - val_loss: 5.4652 - val__perplexity: 4262.0080\n",
      "Epoch 3/10\n",
      "154596/154596 [==============================] - 82s 533us/step - loss: 4.1222 - _perplexity: 369.5797 - val_loss: 5.4723 - val__perplexity: 4448.4314\n",
      "Epoch 4/10\n",
      "154596/154596 [==============================] - 82s 531us/step - loss: 4.0535 - _perplexity: 354.0136 - val_loss: 5.4948 - val__perplexity: 4464.8272\n",
      "Epoch 5/10\n",
      "154596/154596 [==============================] - 82s 533us/step - loss: 4.0050 - _perplexity: 345.0917 - val_loss: 5.5132 - val__perplexity: 4572.0646\n",
      "Epoch 6/10\n",
      "154596/154596 [==============================] - 83s 534us/step - loss: 3.9716 - _perplexity: 340.0346 - val_loss: 5.5267 - val__perplexity: 4544.7840\n",
      "Epoch 7/10\n",
      "154596/154596 [==============================] - 82s 532us/step - loss: 3.9440 - _perplexity: 335.8569 - val_loss: 5.5448 - val__perplexity: 4529.0406\n",
      "Epoch 8/10\n",
      "154596/154596 [==============================] - 82s 532us/step - loss: 3.9241 - _perplexity: 335.3818 - val_loss: 5.5732 - val__perplexity: 4548.5621\n",
      "Epoch 9/10\n",
      "154596/154596 [==============================] - 83s 536us/step - loss: 3.9062 - _perplexity: 332.9441 - val_loss: 5.5797 - val__perplexity: 4679.7269\n",
      "Epoch 10/10\n",
      "154596/154596 [==============================] - 83s 535us/step - loss: 3.8906 - _perplexity: 328.3217 - val_loss: 5.5845 - val__perplexity: 4683.8271\n"
     ]
    }
   ],
   "source": [
    "model.train(X,y, epochs=10, batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'deep . . . . . .'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict('Deep', 6)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dlinto] *",
   "language": "python",
   "name": "conda-env-dlinto-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
