{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\Eli\\Anaconda3\\envs\\dlinto\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Input, GlobalMaxPooling1D\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, LSTM, Bidirectional, CuDNNLSTM\n",
    "from keras.models import Model\n",
    "from keras.initializers import Constant\n",
    "from keras import Sequential\n",
    "import src.dataset as ds\n",
    "import pickle\n",
    "import itertools\n",
    "import random\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORDS_VECTORS_DIR = 'word_vectors/'\n",
    "LYRICS_DIR = 'Data/'\n",
    "GLOVE_DIR = os.path.join(WORDS_VECTORS_DIR, 'glove.6B')\n",
    "TEXT_DATA = os.path.join(LYRICS_DIR, 'unified_lyrics_dump.txt')\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 1 # During each step of the training phase, your architecture will receive as input one word of the lyrics.\n",
    "EMBEDDING_DIM = 300\n",
    "VALIDATION_SPLIT = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(embeddings='glove'):\n",
    "    with open(os.path.join(WORDS_VECTORS_DIR, f'{embeddings}_embeddings.pickle'), 'rb') as f:\n",
    "        pretrained_embeddings = pickle.load(f)\n",
    "    return pretrained_embeddings\n",
    "pretrained_embeddings = load_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 615/615 [00:01<00:00, 369.44it/s]\n"
     ]
    }
   ],
   "source": [
    "X, y = ds.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(TEXT_DATA, 'r') as f:\n",
    "    text = f.read()\n",
    "# finally, vectorize the text samples into a 2D integer tensor\n",
    "tokenizer = Tokenizer(filters='')\n",
    "tokenizer.fit_on_texts([text])\n",
    "# sequences = tokenizer.texts_to_sequences([text])\n",
    "X = [lst[0] for lst in tokenizer.texts_to_sequences(X)] \n",
    "y = [lst[0] for lst in tokenizer.texts_to_sequences(y)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 7506\n"
     ]
    }
   ],
   "source": [
    "# prepare embedding matrix\n",
    "word_index = tokenizer.word_index\n",
    "num_words = len(word_index) + 1\n",
    "print('Vocabulary Size: {}'.format(num_words))\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "not_found = []\n",
    "for word, i in word_index.items(): # Todo: check also word in capitlal (for word2vec)\n",
    "    word_encode = word.encode()\n",
    "    embedding_vector = pretrained_embeddings.get(word_encode)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        not_found.append(word) # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = to_categorical(y, num_classes=num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pre-trained word embeddings into an Embedding layer\n",
    "# note that we set trainable = False so as to keep the embeddings fixed\n",
    "embedding_layer = Embedding(num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            embeddings_initializer=Constant(embedding_matrix),\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 1, 300)            2251800   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 100)               140800    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 7506)              758106    \n",
      "=================================================================\n",
      "Total params: 3,150,706\n",
      "Trainable params: 3,150,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(Bidirectional(CuDNNLSTM(50)))\n",
    "model.add(Dense(num_words, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      " - 50s - loss: 5.0781 - acc: 0.1959\n",
      "Epoch 2/5\n",
      " - 45s - loss: 4.4314 - acc: 0.2267\n",
      "Epoch 3/5\n",
      " - 45s - loss: 4.1942 - acc: 0.2366\n",
      "Epoch 4/5\n",
      " - 44s - loss: 4.0581 - acc: 0.2436\n",
      "Epoch 5/5\n",
      " - 45s - loss: 3.9713 - acc: 0.2488\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x24b8b909e10>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compile network\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit network\n",
    "model.fit(X, y, epochs=5, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a sequence from the model\n",
    "def generate_seq(model, tokenizer, seed_text, n_words):\n",
    "\tin_text, result = seed_text, seed_text\n",
    "\t# generate a fixed number of words\n",
    "\tfor _ in range(n_words):\n",
    "\t\t# encode the text as integer\n",
    "\t\tencoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "\t\tencoded = np.array(encoded)\n",
    "\t\t# predict a word in the vocabulary\n",
    "\t\tyhat = model.predict_classes(encoded, verbose=0)\n",
    "\t\t# map predicted word index to word\n",
    "\t\tout_word = ''\n",
    "\t\tfor word, index in tokenizer.word_index.items():\n",
    "\t\t\tif index == yhat:\n",
    "\t\t\t\tout_word = word\n",
    "\t\t\t\tbreak\n",
    "\t\t# append to input\n",
    "\t\tin_text, result = out_word, result + ' ' + out_word\n",
    "\treturn result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"ball on the way . i 'm gon na be . i 'm gon na be . i 'm gon na be . i 'm gon na be . i 'm gon na be . i 'm gon na be . i 'm gon na be . i 'm gon na\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_seq(model, tokenizer, 'ball', 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check what not exist in our corpus\n",
    "def check_our_corpus(our_words):\n",
    "    not_found = []\n",
    "    for word in our_words:\n",
    "        if word.encode() not in embeddings_index:    \n",
    "            not_found.append(word)\n",
    "    return not_found"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dlinto] *",
   "language": "python",
   "name": "conda-env-dlinto-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
